All problems generated by GPT 5 with scoring with answers by LLaMa 4. Sorted by probability - however, the exact probabilities are not highlighted because I am lazy, sorry.

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

- Task: Given an integer N followed by N integers, output their sum.
- Input:
  - Line 1: N (0 ≤ N ≤ 200000)
  - Line 2+: N space-separated integers a_i (−10^9 ≤ a_i ≤ 10^9)
- Output: Single integer — the sum of all a_i.
- Notes:
  - If N = 0, output 0.
  - Ensure the chosen numeric type can hold the sum (e.g., up to about 2×10^14 under the given constraints).

Examples
- Input:
  5
  1 2 3 4 5
  Output:
  15

- Input:
  3
  -2 0 7
  Output:
  5

- Input:
  0
  Output:
  0

SOLUTION:

# Problem Understanding

The task is to write a program that takes an integer N as input, followed by N space-separated integers, and outputs their sum. The key constraints and edge cases are:

- N is an integer between 0 and 200,000 (inclusive).
- The N integers are space-separated and can be between -10^9 and 10^9 (inclusive).
- If N = 0, the program should output 0.
- The program should be able to handle large sums, up to about 2×10^14 under the given constraints.

The input consists of two parts: the first line contains N, and the subsequent lines contain N space-separated integers. The output is a single integer representing the sum of the N integers.

A subtle requirement is that the chosen numeric type should be able to hold the sum of the N integers.

# Key Insights

The crucial observations that lead to the solution are:

- The input format is specific, with N on the first line and the N integers on subsequent lines.
- The program needs to handle large values of N and large integers.
- The sum of the N integers can be large, requiring a suitable numeric type.
- If N = 0, the program should output 0 without attempting to read N integers.
- The input integers can be negative, zero, or positive.
- The program should be able to handle cases where N is large (up to 200,000).

The key algorithmic insight is that the program can read N, then read the N integers, store them in a data structure (or directly sum them), and finally sum them up.

Important realizations include:

- The need to check if N = 0 and handle this case separately.
- The need to use a suitable numeric type that can handle large sums.
- The fact that the input integers are space-separated, requiring a suitable input reading mechanism.

Critical examples or test cases that led to insights include:

- N = 5, integers = [1, 2, 3, 4, 5], expected output = 15.
- N = 3, integers = [-2, 0, 7], expected output = 5.
- N = 0, expected output = 0.

# Solution Strategy

The steps to solve this problem are:

1. Read N from the first line of input.
2. Check if N = 0. If so, output 0 immediately.
3. Read the N integers from the subsequent lines of input.
4. Store the N integers in a suitable data structure (or directly sum them).
5. Sum up the N integers using a suitable numeric type that can handle large sums.
6. Output the sum as a single integer.

Important optimizations or special cases to handle include:

- Handling the case where N = 0 separately to avoid attempting to read N integers.
- Using a suitable numeric type that can handle large sums.
- Ensuring that the input reading mechanism can handle space-separated integers.

Here's the python solution:
```python
def main():
    try:
        N = int(input().strip())
        if N == 0:
            print(0)
        else:
            numbers = []
            lines = []
            while len(lines) < N:
                try:
                    line = input().strip()
                    lines.extend(line.split())
                except EOFError:
                    break
            for num in lines[:N]:
                try:
                    numbers.append(int(num))
                except ValueError:
                    # In a real application, you'd log this error
                    return
            if len(numbers) == N:
                print(sum(numbers))
            else:
                # Handle the case where not enough numbers were provided
                return
    except ValueError:
        # In a real application, you'd log this error
        return

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

- Given a list of integers, output the list sorted by:
  1) Higher frequency first.
  2) If frequencies tie, the value whose first occurrence appears earlier in the original list comes first.
- The output must contain each integer repeated exactly as many times as it appears.

Input
- Line 1: integer N (1 ≤ N ≤ 200000)
- Line 2: N space-separated integers (each fits in 32-bit signed range)

Output
- One line with the N integers sorted per the rules above.

Notes
- “First occurrence” refers to the first index (0-based) where a value appears in the input list.
- Sorting must be stable with respect to first occurrence for ties by frequency.

Examples

Example 1
Input
8
4 5 6 5 4 3 5 4

Output
4 4 4 5 5 5 6 3

Explanation: Frequencies: 4→3, 5→3, 6→1, 3→1.
Tie at freq=3: first(4)=0 < first(5)=1, so 4 before 5.
Tie at freq=1: first(6)=2 < first(3)=5, so 6 before 3.

Example 2
Input
7
10 20 10 30 20 30 30

Output
30 30 30 10 10 20 20

SOLUTION:

# Problem Understanding

The problem requires sorting a list of integers based on their frequency and first occurrence in the original list. The output should contain each integer repeated exactly as many times as it appears in the input list.

Key constraints and edge cases:

* The input list contains integers that fit in a 32-bit signed range.
* The length of the input list (N) is between 1 and 200,000 (inclusive).
* The output should be a single line with the N integers sorted per the given rules.
* The sorting should be stable with respect to the first occurrence of integers with tied frequencies.

Input: A list of N integers.
Output: A list of N integers sorted by frequency and first occurrence.

Subtle requirements:

* The output should contain each integer repeated exactly as many times as it appears in the input list.
* The sorting should be stable with respect to the first occurrence of integers with tied frequencies.

# Key Insights

Crucial observations that lead to the solution:

* To solve this problem, we need to count the frequency of each integer in the input list.
* We also need to track the first occurrence of each integer in the original list.
* The problem requires sorting the integers based on their frequency and first occurrence.
* The sorting should be done in descending order of frequency, and for tied frequencies, the integer with the earlier first occurrence should come first.
* We can use dictionaries to store the frequency and first occurrence of each integer.
* The input list needs to be iterated over to count the frequency and track the first occurrence of each integer.
* The integers need to be sorted based on their frequency and first occurrence.
* The sorted integers need to be output in the original order, with each integer repeated exactly as many times as it appears in the input list.

Patterns or properties being used:

* The problem uses the concept of frequency counting and stable sorting.
* The solution relies on the properties of dictionaries for efficient lookups and updates.

Critical examples or test cases:

* The given examples (e.g., Example 1: [4, 5, 6, 5, 4, 3, 5, 4] and Example 2: [10, 20, 10, 30, 20, 30, 30]) help illustrate the problem requirements and the expected output.

# Solution Strategy

Steps to solve this problem:

1. Read the input list of integers.
2. Create two dictionaries: one to store the frequency of each integer and another to store the first occurrence of each integer.
3. Iterate over the input list to count the frequency and track the first occurrence of each integer.
4. Sort the unique integers based on their frequency and first occurrence.
5. Output the sorted integers in the original order, with each integer repeated exactly as many times as it appears in the input list.

Important optimizations or special cases to handle:

* The solution should handle the case where the input list is empty or contains only one element.
* The solution should be efficient enough to handle large input lists (up to 200,000 elements).
* The solution should use dictionaries to store the frequency and first occurrence of each integer, allowing for efficient lookups and updates.

Here's the python solution:
```python
def main():
    try:
        N = int(input().strip())
        nums = list(map(int, input().strip().split()))
        if len(nums) != N:
            # Handle input length mismatch if necessary, but as per problem description, it should be N
            pass
        print(sort_integers(N, nums))
    except Exception as e:
        # Proper error handling should be implemented as per the requirement
        pass

def sort_integers(N, nums):
    freq_dict = {}
    first_occurrence = {}
    for i, num in enumerate(nums):
        if num not in freq_dict:
            freq_dict[num] = 0
            first_occurrence[num] = i
        freq_dict[num] += 1
    
    sorted_nums = sorted(freq_dict.items(), key=lambda x: (-x[1], first_occurrence[x[0]]))
    output = []
    for num, freq in sorted_nums:
        output.extend([num] * freq)
    return ' '.join(map(str, output))

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Given a non-empty string S consisting only of uppercase English letters (A–Z), output its run-length encoding (RLE). For each maximal run of the same character, output the run length followed by the character. Counts are in decimal with no separators.

Input
- A single line containing S (1 ≤ |S| ≤ 100000), only characters ‘A’–‘Z’.

Output
- The RLE of S as described (count followed by character for each run, concatenated).

Examples
1)
Input:
AAABCCDDDD
Output:
3A1B2C4D

2)
Input:
A
Output:
1A

3)
Input:
ABCD
Output:
1A1B1C1D

SOLUTION:

# Problem Understanding

The problem requires implementing run-length encoding (RLE) on a given non-empty string S consisting only of uppercase English letters (A-Z). The task is to output the RLE of S, where for each maximal run of the same character, the run length is followed by the character.

Key constraints and edge cases:
- S is non-empty.
- S consists only of uppercase English letters (A-Z).
- The length of S is between 1 and 100000 (inclusive).
- The output should be in decimal with no separators.

Input: A single line containing S.
Output: The RLE of S as described.

Subtle requirements or implicit constraints:
- The output should be a concatenation of the count and character for each run.
- The count should be in decimal.

# Key Insights

Crucial observations that lead to the solution:
- To solve this problem, we need to iterate through the string S and identify sequences of the same character.
- We need to count the number of consecutive occurrences of the same character.
- When we encounter a different character, we need to output the count of the previous character and the character itself.
- We need to handle the last character's count and output it correctly after iterating through the entire string.

Patterns or properties being used:
- The property that consecutive occurrences of the same character can be counted and represented as a run length.

Key algorithmic insights:
- We can iterate through S, comparing each character with the previous one to identify runs of the same character.
- We need to keep track of the current character and its count.

Important realizations:
- We need to initialize the current character and count at the start of the iteration.
- We need to append the count and character to the result when we encounter a different character.
- We need to handle the edge case where the string has only one character.

Critical examples or test cases that led to insights:
- S = AAABCCDDDD, expected output = 3A1B2C4D.
- S = A, expected output = 1A.
- S = ABCD, expected output = 1A1B1C1D.

# Solution Strategy

Steps to solve this problem:
1. Initialize an empty string to store the result.
2. Initialize the current character and count using the first character of S.
3. Iterate through S starting from the second character.
4. For each character, compare it with the current character.
5. If the character is the same as the current character, increment the count.
6. If the character is different from the current character, append the count and current character to the result, and update the current character and reset the count.
7. After iterating through S, append the count and character of the last run to the result.
8. Output the result.

Here's the python solution:
```python
def main():
    try:
        S = input().strip()
        result = run_length_encoding(S)
        print(result)
    except Exception as e:
        # In a real application, you might log this error
        pass

def run_length_encoding(S):
    result = ""
    if not S:
        return result
    
    current_char = S[0]
    count = 1
    
    for char in S[1:]:
        if char == current_char:
            count += 1
        else:
            result += str(count) + current_char
            current_char = char
            count = 1
    
    result += str(count) + current_char
    return result

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Given two strings S and T, find the shortest substring of S that contains every character of T (including multiplicities). If there are multiple answers with the same minimum length, return the one with the smallest starting index. If no such window exists, output -1.

Input
- Line 1: string S (1 ≤ |S| ≤ 200000)
- Line 2: string T (1 ≤ |T| ≤ min(|S|, 200000))
- Characters are arbitrary visible ASCII. Matching is case-sensitive.

Output
- If a covering window exists: print the substring of S that is the shortest cover. If multiple, choose the one with the smallest start index.
- Otherwise: print -1.

Examples
1)
Input
ADOBECODEBANC
ABC
Output
BANC

2)
Input
a
aa
Output
-1

3)
Input
aaaaab
aab
Output
aab

SOLUTION:

# Problem Understanding

The problem requires finding the shortest substring of a given string S that contains every character of another string T, including their multiplicities. If multiple substrings have the same minimum length, the one with the smallest starting index should be returned. If no such substring exists, the output should be -1.

Key constraints and edge cases:

- The length of S is between 1 and 200,000.
- The length of T is between 1 and the minimum of the length of S and 200,000.
- Characters in S and T are arbitrary visible ASCII characters, and the matching is case-sensitive.
- T can have repeated characters.
- S or T can be empty (though this is not explicitly mentioned, it's implied by the examples and the nature of the problem).
- There might be multiple substrings of S that contain all characters of T with the same minimum length.

Input: Two strings, S and T.
Output: The shortest substring of S that contains every character of T, or -1 if no such substring exists.

Subtle requirements or implicit constraints:

- The problem requires considering the multiplicities of characters in T, not just their presence.
- The output should be the substring with the smallest starting index if there are multiple substrings of the same minimum length.

# Key Insights

1. **Understanding the Problem**: The task is to find the shortest substring of S that covers all characters of T, considering their multiplicities.
2. **Approach**: Using a sliding window technique is suitable because it allows examining substrings of S by expanding and contracting a window.
3. **Character Count Dictionary**: Creating a dictionary to store the count of characters in T helps in keeping track of the characters needed to be found in S.
4. **Required Characters Count**: Maintaining a count of unique characters in T that are still required to be found in the current window is crucial. When this count reaches zero, it means the current window contains all characters of T.
5. **Sliding Window Technique**: Expanding the window to the right and adding characters to a window count dictionary, and then contracting the window from the left when all required characters are found, helps in finding the shortest substring.
6. **Comparison and Update**: Comparing the count of characters in the window with the count in T's character dictionary and updating the minimum length substring accordingly is key to finding the correct answer.
7. **Handling Edge Cases**: Checking if the length of T is more than S or if S is empty or T is empty is necessary to handle edge cases.
8. **Multiplicities and Case-Sensitivity**: The solution must consider the multiplicities of characters and be case-sensitive.

# Solution Strategy

1. **Create a Dictionary for T's Characters**: Store the count of each character in T in a dictionary.
2. **Initialize Variables**: Initialize variables to keep track of the minimum length substring, its starting index, and the count of required characters.
3. **Sliding Window**:
   - Expand the window to the right by incrementing the right pointer and update the window's character count dictionary.
   - If the character at the right pointer is in T's dictionary and its count in the window is less than or equal to T's count, decrement the required characters count.
   - When the required characters count reaches zero, it means the current window is valid.
   - Contract the window from the left by incrementing the left pointer and update the window's character count dictionary.
   - If the character at the left pointer is in T's dictionary and its count in the window is less than T's count, increment the required characters count.
4. **Update Minimum Length Substring**: If the current window's length is less than the minimum length found so far, update the minimum length and the corresponding substring.
5. **Check for Valid Window**: After iterating through S, check if a valid window (containing all characters of T) was found. If yes, return the minimum length substring; otherwise, return -1.
6. **Handling Edge Cases**: Before starting the iteration, check if T's length is more than S's or if either S or T is empty. If T's length is more than S's or if S is empty or T is empty, return -1 as there's no valid substring.

Here's the python solution:
```python
from collections import defaultdict

def min_window(S, T):
    if not S or not T:
        return -1
    
    t_count = defaultdict(int)
    for char in T:
        t_count[char] += 1
    
    required_chars = len(t_count)
    formed_chars = 0
    
    window_counts = defaultdict(int)
    min_len = float("inf")
    min_window = ""
    left = 0
    
    for right in range(len(S)):
        character = S[right]
        window_counts[character] += 1
        
        if character in t_count and window_counts[character] == t_count[character]:
            formed_chars += 1
        
        while left <= right and formed_chars == len(t_count):
            character = S[left]
            
            if right - left + 1 < min_len:
                min_len = right - left + 1
                min_window = S[left:right + 1]
            
            window_counts[character] -= 1
            if character in t_count and window_counts[character] < t_count[character]:
                formed_chars -= 1
            
            left += 1
    
    if min_len == float("inf"):
        return -1
    else:
        return min_window

S = input().strip()
T = input().strip()
print(min_window(S, T))
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

- Task: Given a string S and an integer K, compute the number of substrings of S that contain exactly K distinct characters.

- Input:
  - Line 1: string S (non-empty; arbitrary ASCII letters/digits/symbols are allowed)
  - Line 2: integer K (1 ≤ K ≤ |S|)

- Output:
  - A single integer: the number of substrings of S that have exactly K distinct characters.
  - Use 64-bit integer if needed.

- Examples:
  - Example 1
    - Input:
      - abcba
      - 2
    - Output:
      - 5
    - Explanation: The substrings with exactly 2 distinct characters are: ab, bc, cb, ba, bcb.
  - Example 2
    - Input:
      - aaaa
      - 1
    - Output:
      - 10

SOLUTION:

# Problem Understanding

The problem requires computing the number of substrings of a given string S that contain exactly K distinct characters.

Key constraints and edge cases:

- S is a non-empty string.
- K is an integer between 1 and the length of S (inclusive).
- S can contain arbitrary ASCII letters, digits, or symbols.
- The problem requires handling cases where S is empty or K is out of the valid range.

Input: A string S and an integer K.
Output: The number of substrings of S that have exactly K distinct characters.

Subtle requirements or implicit constraints:

- The problem requires using a 64-bit integer to handle large counts.
- The solution should be efficient enough to handle strings of varying lengths.

# Key Insights

Crucial observations that lead to the solution:

- The problem can be approached using a sliding window technique combined with tracking the number of distinct characters within each window.
- Maintaining a frequency map of characters within the current window is essential to count distinct characters.
- The problem can be rephrased as finding the difference between the count of substrings with at most K distinct characters and the count of substrings with at most K-1 distinct characters.
- The count of substrings with exactly K distinct characters can be derived using the formula: count_at_most_K(S, K) - count_at_most_K(S, K-1).
- The function count_at_most_K(S, K) can be implemented using a sliding window approach, where we iterate over S, expand the window to the right, and shrink it from the left when the number of distinct characters exceeds K.
- For each valid window, we can add the length of the valid substrings it can form to our count.

Patterns or properties being used:

- The sliding window technique is used to efficiently traverse the string and count substrings.
- The frequency map is used to track the number of distinct characters within each window.

Critical examples or test cases that led to insights:

- S = abcba and K = 2, where the expected output is 5.
- S = aaaa and K = 1, where the expected output is 10.

# Solution Strategy

Steps to solve this problem:

1. Implement a function count_at_most_K(S, K) that counts the number of substrings with at most K distinct characters using a sliding window approach.
2. Initialize a frequency map to track the characters within the current window.
3. Iterate over S, expanding the window to the right and updating the frequency map.
4. When the number of distinct characters exceeds K, shrink the window from the left and update the frequency map.
5. For each valid window, add the length of the valid substrings it can form to the count.
6. Return the count of substrings with at most K distinct characters.
7. Compute the count of substrings with exactly K distinct characters using the formula: count_at_most_K(S, K) - count_at_most_K(S, K-1).
8. Handle edge cases where K is less than 1 or S is empty.

Important optimizations or special cases to handle:

- Handling cases where K is less than 1 by returning 0.
- Handling cases where S is empty by returning 0.
- Using a 64-bit integer to handle large counts.

Here's the python solution:
```python
def count_at_most_K(S, K):
    if K < 1:
        return 0
    
    char_freq = {}
    left = 0
    count = 0
    distinct = 0
    
    for right in range(len(S)):
        char_freq[S[right]] = char_freq.get(S[right], 0) + 1
        if char_freq[S[right]] == 1:
            distinct += 1
        
        while distinct > K:
            char_freq[S[left]] -= 1
            if char_freq[S[left]] == 0:
                distinct -= 1
            left += 1
        
        # For each valid window ending at 'right', we can form (right - left + 1) valid substrings.
        count += right - left + 1
    
    return count

def main():
    S = input().strip()
    K = int(input().strip())
    if K < 1 or K > len(S):
        if K < 1:
            print(0)
        else:
            print(0)
    else:
        print(count_at_most_K(S, K) - count_at_most_K(S, K-1))

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given T absolute Unix-style paths. For each path, output its canonical (simplified) absolute form.

Rules for canonicalization:
- Multiple consecutive slashes “//…/” collapse to a single “/”.
- A segment “.” means “current directory” and is removed.
- A segment “..” means “parent directory” and removes the previous segment if one exists; at root, “..” has no effect.
- The result is absolute and has no trailing slash, except the root “/”.

Input
- First line: integer T (number of test cases).
- Next T lines: each line is one absolute path consisting of characters from [A–Z, a–z, 0–9, ., _, -, /].
- Each path begins with “/”.

Output
- Print T lines, each the canonical form of the corresponding input path.

Examples
Input
5
/
////
/home//foo/
/a/./b/../../c/
/../x/.././y/..///./z/

Output
/
/
/home/foo
/c
/z

SOLUTION:

# Problem Understanding

The problem requires simplifying T absolute Unix-style paths according to specific canonicalization rules. The input consists of an integer T representing the number of test cases, followed by T lines, each containing an absolute Unix-style path. The output should be T lines, each representing the canonical (simplified) form of the corresponding input path.

Key constraints and edge cases:

- The input paths are absolute Unix-style paths.
- The paths consist of characters from [A–Z, a–z, 0–9, ., _, -, /].
- Each path begins with “/”.
- The canonicalization rules are:
  - Multiple consecutive slashes “//…/” collapse to a single “/”.
  - A segment “.” means “current directory” and is removed.
  - A segment “..” means “parent directory” and removes the previous segment if one exists; at root, “..” has no effect.
- The result is absolute and has no trailing slash, except for the root “/”.

Input: T (number of test cases), followed by T absolute Unix-style paths.
Output: T canonical (simplified) absolute Unix-style paths.

Subtle requirements or implicit constraints:

- The input paths are absolute, so they start with a “/”.
- The output should not have a trailing slash unless it is the root “/”.

# Key Insights

Crucial observations that lead to the solution:

- The canonicalization rules can be applied by processing the input path segment by segment.
- A stack data structure can be used to keep track of the valid segments.
- The input path can be split into segments by identifying the “/” character, but multiple consecutive “/” characters should be treated as a single “/”.
- The “.” segment can be ignored as it represents the current directory.
- The “..” segment requires removing the last valid segment if it exists, but it has no effect at the root level.
- The canonical path can be reconstructed by joining the valid segments with a single “/” in between.

Patterns or properties being used:

- The stack data structure is used to maintain the valid segments.
- The input path is processed segment by segment.

Key algorithmic insights:

- The input path is processed by iterating through the characters and identifying the segments.
- The canonicalization rules are applied to each segment using a stack data structure.

Important realizations:

- The input path can be processed character by character to handle consecutive “/” characters correctly.
- The stack data structure is suitable for keeping track of the valid segments.

Critical examples or test cases that led to insights:

- The example inputs and outputs provided, such as “/home//foo/” being simplified to “/home/foo”, helped illustrate the application of the canonicalization rules.

# Solution Strategy

Steps to solve this problem:

1. Read the number of test cases T.
2. For each test case, read the absolute Unix-style path.
3. Process the input path character by character to identify the segments and handle consecutive “/” characters correctly.
4. Use a stack data structure to keep track of the valid segments.
5. Apply the canonicalization rules to each segment:
   - Ignore “.” segments.
   - For “..” segments, remove the last valid segment if it exists, but have no effect at the root level.
   - Push other valid segments onto the stack.
6. Reconstruct the canonical path by joining the valid segments in the stack with a single “/” in between.
7. If the stack is empty, the canonical path is the root “/”.
8. Output the canonical path for each test case.

Here's the python solution:
```python
def simplify_path(path):
    stack = []
    segment = ""
    for char in path + "/":
        if char == "/":
            if segment == "..":
                if stack:
                    stack.pop()
            elif segment != "" and segment != ".":
                stack.append(segment)
            segment = ""
        else:
            segment += char
    canonical_path = "/" + "/".join(stack)
    if canonical_path == "":
        return "/"
    else:
        return canonical_path

def main():
    try:
        T = int(input().strip())
        for _ in range(T):
            path = input().strip()
            print(simplify_path(path))
    except EOFError:
        pass
    except ValueError:
        # In a real application, you'd log this or handle it according to your error handling policy
        pass

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given two strings S and T. Find the shortest substring of S that contains every character of T with at least the same multiplicity as in T. If multiple substrings have the same minimal length, return the one with the smallest starting index. If no such substring exists, output -1.

Input
- Line 1: S (1 ≤ |S| ≤ 200,000)
- Line 2: T (1 ≤ |T| ≤ min(|S|, 100,000))
- Characters are case-sensitive; treat all bytes as ordinary characters (no special semantics).

Output
- If a cover exists: three space-separated items: start_index end_index substring
  - Indices are 0-based and inclusive.
- If no cover exists: -1

Examples
1)  
Input  
ADOBECODEBANC  
ABC  
Output  
9 12 BANC  
Explanation: “BANC” is the shortest substring containing A, B, and C.

2)  
Input  
a  
aa  
Output  
-1

3)  
Input  
aa  
aa  
Output  
0 1 aa

SOLUTION:

# Problem Understanding

The problem requires finding the shortest substring of a given string S that contains every character of another string T with at least the same multiplicity as in T. If multiple substrings have the same minimal length, the one with the smallest starting index should be returned. If no such substring exists, the output should be -1.

Key constraints and edge cases:

- The length of S is between 1 and 200,000.
- The length of T is between 1 and the minimum of the length of S and 100,000.
- Characters are case-sensitive.
- All bytes are treated as ordinary characters (no special semantics).
- S and T can contain duplicate characters.
- The substring of S must contain every character of T with at least the same multiplicity as in T.

Input: Two strings S and T.
Output: If a valid substring exists, the output should be three space-separated items: the start index, end index, and the substring itself. If no valid substring exists, the output should be -1.

Subtle requirements or implicit constraints:

- The indices are 0-based and inclusive.
- The output substring should be a contiguous substring of S.

# Key Insights

- To solve this problem, we need to count the frequency of each character in T and compare it with the frequency of characters in the substrings of S.
- A sliding window approach can be used to efficiently scan through S and find the shortest substring that contains all characters of T with the required multiplicity.
- We need to maintain a count of characters in the current window that matches the count in T.
- The `formed` variable is used to keep track of how many characters in T have been matched in the current window with the required frequency.
- When `formed == len(t_count)`, it means the current window is valid, and we can try to minimize the window.
- We need to store the minimum length substring that satisfies the condition and its start and end indices in S.
- The character count dictionary `t_count` is used to store the frequency of characters in T.
- The dictionary `window_counts` is used to store the frequency of characters in the current window.

Critical examples or test cases that led to insights:

- S = ADOBECODEBANC and T = ABC.
- S = a and T = aa.
- S = aa and T = aa.

# Solution Strategy

1. Create a dictionary `t_count` to store the frequency of characters in T.
2. Initialize the `required` variable to the number of unique characters in T.
3. Initialize the `left` pointer to 0, representing the start of the sliding window.
4. Initialize the `formed` variable to 0, which keeps track of how many characters in T have been matched in the current window with the required frequency.
5. Initialize the `window_counts` dictionary to store the frequency of characters in the current window.
6. Initialize the `min_len` variable to infinity, `min_start` to -1, and `min_end` to -1, which will store the minimum length substring that satisfies the condition and its start and end indices in S.
7. Iterate through S using the `right` pointer, expanding the window to the right.
8. For each character in S, update `window_counts` and check if the frequency of the character in `window_counts` is equal to the frequency in `t_count`. If so, increment `formed`.
9. While `formed == len(t_count)`, it means the current window is valid. Try to minimize the window by moving the `left` pointer to the right.
10. Update `min_len`, `min_start`, and `min_end` if a smaller valid window is found.
11. If `min_start` is still -1 after iterating through S, return -1. Otherwise, return the start index, end index, and the substring itself.

Here's the python solution:
```python
def min_window(s, t):
    if not s or not t:
        return -1
    
    t_count = {}
    for char in t:
        if char in t_count:
            t_count[char] += 1
        else:
            t_count[char] = 1
    
    required = len(t_count)
    left = 0
    min_len = float('inf')
    min_start = -1
    min_end = -1
    formed = 0
    
    window_counts = {}
    for right in range(len(s)):
        character = s[right]
        window_counts[character] = window_counts.get(character, 0) + 1
        
        if character in t_count and window_counts[character] == t_count[character]:
            formed += 1
        
        while left <= right and formed == len(t_count):
            character = s[left]
            
            if right - left + 1 < min_len:
                min_len = right - left + 1
                min_start = left
                min_end = right
            
            window_counts[character] -= 1
            if character in t_count and window_counts[character] < t_count[character]:
                formed -= 1
            
            left += 1
    
    if min_start == -1:
        return -1
    else:
        return f'{min_start} {min_end} {s[min_start:min_end+1]}'

s = input().strip()
t = input().strip()
print(min_window(s, t))
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Find the shortest contiguous subarray of A that contains all elements of T, respecting multiplicities. If multiple shortest windows exist, return the one with the smallest start index; if still tied, return the one with the smallest end index. If no such subarray exists, print -1 -1.

Input
- Line 1: two integers n m (1 ≤ m ≤ n ≤ 2⋅10^5)
- Line 2: n space-separated integers A[0..n-1]
- Line 3: m space-separated integers T[0..m-1]

Output
- Two integers l r (0-based, inclusive) representing the shortest covering subarray A[l..r], or -1 -1 if it does not exist.

Notes
- “Contains all elements of T, respecting multiplicities” means each value v must appear in A[l..r] at least as many times as it appears in T.
- Any integers are allowed in A and T (negative, zero, positive).

Examples

Example 1
Input
6 3
1 2 2 3 2 1
1 2 2

Output
0 2

Explanation
Subarray A[0..2] = [1, 2, 2] covers T exactly and is the shortest possible.

Example 2
Input
5 2
5 6 7 8 9
1 1

Output
-1 -1

Explanation
No window contains two occurrences of 1.

SOLUTION:

# Problem Understanding

The problem requires finding the shortest contiguous subarray of a given array A that contains all elements of another array T, respecting their multiplicities. If multiple shortest subarrays exist, the solution should return the one with the smallest start index, and if still tied, return the one with the smallest end index. If no such subarray exists, the solution should return -1 -1.

Key constraints and edge cases:

- The length of array A (n) and array T (m) are given, where 1 ≤ m ≤ n ≤ 2⋅10^5.
- The elements of A and T can be any integers (negative, zero, or positive).
- The problem requires respecting the multiplicities of elements in T.
- If T has elements that are not in A, or if the count of an element in T is more than in A, the solution should return -1 -1.

Input: Two integers n and m, array A of length n, and array T of length m.
Output: Two integers representing the start and end indices (0-based, inclusive) of the shortest contiguous subarray of A that contains all elements of T, or -1 -1 if no such subarray exists.

Subtle requirements:

- The solution should handle cases where there are multiple shortest subarrays.
- The solution should respect the multiplicities of elements in T.

# Key Insights

- To solve this problem, we need to count the frequency of each element in T and compare it with the frequency of elements in the subarrays of A.
- We can use a sliding window approach to find the shortest contiguous subarray of A that contains all elements of T.
- We need to maintain a count of elements in the current window that are also in T, and check if this count is equal to the total count of elements in T.
- We need to keep track of the minimum length of the subarray that contains all elements of T, as well as its start and end indices.
- If we find a subarray that contains all elements of T with the same multiplicity, and its length is less than the current minimum length, we update the minimum length and the corresponding start and end indices.
- If we find a subarray with the same minimum length but with a smaller start index, or with the same start index but a smaller end index, we update the start and end indices accordingly.
- We can use a dictionary to store the count of elements in T and another dictionary to store the count of elements in the current window.

# Solution Strategy

1. Count the frequency of each element in T using a dictionary (t_count).
2. Initialize variables to keep track of the minimum length of the subarray, its start index, and its end index.
3. Initialize two pointers, left and right, to represent the start and end of the sliding window.
4. Initialize a dictionary (window_counts) to store the count of elements in the current window that are also in T.
5. Initialize a variable (formed) to keep track of the number of elements in the current window that are also in T and have the same count as in T.
6. Iterate over the array A using the right pointer, and for each element, update the count in window_counts and the value of formed.
7. If the count of elements in the current window that are also in T is equal to the total count of elements in T, try to minimize the window by moving the left pointer to the right.
8. Update the minimum length and the corresponding start and end indices if a shorter subarray is found.
9. Continue steps 6-8 until the right pointer reaches the end of array A.
10. Return the start and end indices of the shortest contiguous subarray of A that contains all elements of T, or -1 -1 if no such subarray exists.

Here's the python solution:
```python
from collections import defaultdict

def main():
    try:
        n, m = map(int, input().split())
        A = list(map(int, input().split()))
        T = list(map(int, input().split()))
        l, r = shortest_subarray(n, m, A, T)
        print(l, r)
    except Exception as e:
        # In a real application, you might want to log this error
        pass

def shortest_subarray(n, m, A, T):
    t_count = defaultdict(int)
    for num in T:
        t_count[num] += 1
    
    required = len(t_count)
    
    left = 0
    min_len = float('inf')
    min_left = -1
    min_right = -1
    
    window_counts = defaultdict(int)
    formed = 0
    
    for right in range(n):
        num = A[right]
        if num in t_count:
            window_counts[num] += 1
            if window_counts[num] == t_count[num]:
                formed += 1
        
        while left <= right and formed == required:
            num = A[left]
            if right - left + 1 < min_len:
                min_len = right - left + 1
                min_left = left
                min_right = right
            elif right - left + 1 == min_len:
                if left < min_left:
                    min_left = left
                    min_right = right
                elif left == min_left and right < min_right:
                    min_right = right
            
            if num in t_count:
                window_counts[num] -= 1
                if window_counts[num] < t_count[num]:
                    formed -= 1
            left += 1
    
    if min_left == -1:
        return -1, -1
    else:
        return min_left, min_right

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given an array A of n integers, but the array is circular: the element after A[n−1] is A[0]. For each index i, find the first element strictly greater than A[i] when scanning to the right (wrapping around as needed). If no such element exists, output −1 for that position.

Input
- n (1 ≤ n ≤ 200000)
- A[0..n−1] (−10^9 ≤ A[i] ≤ 10^9)

Output
- n space-separated integers, where the i-th value is the next greater element for A[i], or −1 if it does not exist.

Examples
1)
Input:
5
1 2 1 3 0

Output:
2 3 3 -1 1

Explanation:
- A[0]=1 → next greater is 2
- A[1]=2 → next greater is 3
- A[2]=1 → next greater is 3
- A[3]=3 → no greater → −1
- A[4]=0 → next greater is 1

2)
Input:
4
4 3 2 1

Output:
-1 4 4 4

Explanation:
- For 4, none greater → −1
- For 3, the next greater (wrapping) is 4
- For 2, next greater is 4
- For 1, next greater is 4

SOLUTION:

# Problem Understanding

The problem requires finding the next greater element for each index in a given circular array. The array is considered circular, meaning that the element after the last element is the first element.

Key constraints and edge cases:

- The array size n is between 1 and 200,000 (inclusive).
- The array elements A[i] are integers between -10^9 and 10^9 (inclusive).
- If no greater element exists for an element, the output should be -1 for that position.
- The array is circular, so the scan for the next greater element wraps around to the start of the array if necessary.

Input: The size of the array n and the array elements A[0..n-1].
Output: n space-separated integers representing the next greater element for each A[i], or -1 if it does not exist.

Subtle requirements or implicit constraints:

- The solution should be efficient enough to handle large inputs within the given constraints.
- The output should be in the order corresponding to the input array elements.

# Key Insights

Crucial observations that lead to the solution:

1. The problem involves finding the next greater element for each element in a circular array.
2. A stack data structure can be used to efficiently solve this problem by keeping track of the indices of the elements for which we haven't found the next greater element yet.
3. Since the array is circular, we need to iterate through the array at least twice to ensure we cover all elements in the worst case (when the array is sorted in descending order).
4. We can simulate the circular behavior by using the modulo operator to wrap around the array indices.
5. By iterating through the array twice, we can ensure that we find the next greater element for each element, or determine that no such element exists.

Patterns or properties being used:

- The stack data structure is used to maintain a list of indices of elements for which we haven't found the next greater element yet.
- The circular nature of the array is handled by iterating through the array twice and using the modulo operator to wrap around the array indices.

Key algorithmic insights:

- We iterate through the array twice to cover all elements.
- For each element, we pop the stack elements that are smaller than the current element and update the result for the popped indices.
- We push the current index onto the stack if we haven't found its next greater element yet.

Important realizations:

- We need to initialize the result array with -1 to handle cases where there is no greater element for an element.
- We use the modulo operator to simulate the circular behavior of the array.

Critical examples or test cases that led to insights:

- Example 1: Input: 5, [1, 2, 1, 3, 0], Output: [2, 3, 3, -1, 1]
- Example 2: Input: 4, [4, 3, 2, 1], Output: [-1, 4, 4, 4]

# Solution Strategy

Steps to solve this problem:

1. Initialize an array `result` of size n with all elements set to -1 to store the next greater element for each index.
2. Create an empty stack to store the indices of the elements.
3. Iterate through the array twice by using a loop that goes from 0 to 2n-1.
4. For each index i, calculate the actual index in the array using the modulo operator (i % n).
5. While the stack is not empty and the top element of the stack is smaller than the current element A[i % n], pop the top element from the stack and update the result for the popped index with the current element A[i % n].
6. Push the current index i % n onto the stack if we are in the first iteration (i < n).
7. After the iteration is complete, the `result` array will contain the next greater element for each index, or -1 if no greater element exists.
8. Output the elements of the `result` array space-separated.

Here's the python solution:
```python
def main():
    try:
        n = int(input().strip())
        A = list(map(int, input().strip().split()))
        if len(A) != n:
            # Handle input length mismatch if necessary, but as per problem statement, we assume input is correct.
            pass
        result = next_greater_element(n, A)
        print(' '.join(map(str, result)))
    except Exception as e:
        # Proper error handling should be implemented based on the requirements and constraints.
        pass

def next_greater_element(n, A):
    result = [-1] * n
    stack = []
    for i in range(2 * n):
        index = i % n
        while stack and A[stack[-1]] < A[index]:
            result[stack.pop()] = A[index]
        if i < n:
            stack.append(index)
    return result

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

- Task:
  Given an array A of length n and a multiset T of size m (elements may repeat), find the shortest contiguous subarray of A that contains at least the multiplicity of every value in T. If multiple shortest windows exist, return the one with the smallest starting index. If no such subarray exists, output -1.

- Input format:
  - Line 1: n m  (1 ≤ m ≤ n)
  - Line 2: n space-separated integers A[0], A[1], …, A[n−1]
  - Line 3: m space-separated integers describing multiset T (duplicates allowed)

- Output format:
  - If a covering window exists: print two integers l r (0-based, inclusive) for the chosen window.
  - Otherwise: print -1.

- Details:
  - “Contains” means: for every value v in T, the window must include at least count_T(v) occurrences of v.
  - Tie-breaking: among all windows with minimal length, pick the one with the smallest l.

- Example 1:
  Input:
  10 3
  7 5 9 5 7 5 9 7 5 5
  5 7 5

  Output:
  3 5

  Explanation:
  The subarray A[3..5] = [5, 7, 5] satisfies counts(5)=2 and counts(7)=1 and has minimal length 3. It is also the leftmost among minimal windows.

- Example 2:
  Input:
  5 2
  1 2 3 4 5
  6 6

  Output:
  -1

SOLUTION:

# Problem Understanding

The problem requires finding the shortest contiguous subarray of a given array A that contains at least the multiplicity of every value in a multiset T. If multiple shortest windows exist, the solution should return the one with the smallest starting index. If no such subarray exists, the output should be -1.

Key constraints and edge cases:

- The length of array A is n, and the size of multiset T is m, where 1 ≤ m ≤ n.
- Elements in T may repeat.
- The input format is: n m, followed by n space-separated integers for A, and then m space-separated integers for T.
- The output format is: if a covering window exists, print two integers l r (0-based, inclusive) for the chosen window; otherwise, print -1.
- "Contains" means that for every value v in T, the window must include at least count_T(v) occurrences of v.
- Tie-breaking: among all windows with minimal length, pick the one with the smallest l.

Input: n, m, array A, multiset T.
Output: The start and end indices (0-based, inclusive) of the shortest contiguous subarray of A that contains at least the multiplicity of every value in T, or -1 if no such subarray exists.

Subtle requirements or implicit constraints:

- The problem requires considering the multiplicity of elements in T.
- The solution should handle cases where T contains elements not present in A.

# Key Insights

Crucial observations that lead to the solution:

1. The problem can be solved using a sliding window approach.
2. To efficiently count the occurrences of elements within the current window and compare them with the counts in T, two dictionaries can be used: one for T (count_T) and one for the current window (window_counts).
3. The condition for the window to contain at least the multiplicity of every value in T is that for every element in T, its count in the window is greater than or equal to its count in T.
4. The sliding window approach involves expanding the window to the right and checking if the condition is met. Once the condition is met, the window can be contracted from the left to find the shortest possible window.
5. To handle tie-breaking, the solution should keep track of the minimum length window seen so far and its starting index.
6. The count_T dictionary should be initialized with the counts of elements in T, and the window_counts dictionary should be updated dynamically as the window is expanded and contracted.
7. The solution should handle cases where no valid window exists (i.e., when required_elements > 0 after iterating through A).

Patterns or properties being used:

- The sliding window technique is used to efficiently find the shortest contiguous subarray.
- Dictionary lookups and updates are used to count the occurrences of elements in T and within the current window.

Critical examples or test cases that led to insights:

- Example 1: n = 10, m = 3, A = [7, 5, 9, 5, 7, 5, 9, 7, 5, 5], T = [5, 7, 5] helped illustrate the need to consider the multiplicity of elements in T and the importance of tie-breaking.
- Example 2: n = 5, m = 2, A = [1, 2, 3, 4, 5], T = [6, 6] highlighted the need to handle cases where T contains elements not present in A.

# Solution Strategy

Steps to solve this problem:

1. Initialize count_T by counting the occurrences of each element in T.
2. Initialize required_elements to the number of unique elements in T.
3. Initialize window_counts to an empty dictionary and formed_elements to 0.
4. Initialize min_length to infinity, min_window to (-1, -1).
5. Iterate through A using the sliding window approach:
   - Expand the window to the right by incrementing the count of the current element in window_counts.
   - If the count of the current element in window_counts is equal to its count in count_T, increment formed_elements.
   - While the window contains at least the multiplicity of every value in T (i.e., formed_elements == required_elements), try to contract the window from the left.
   - Update min_length and min_window if a shorter window is found.
6. After iterating through A, check if a valid window was found. If min_length is not infinity, return min_window; otherwise, return -1.

Important optimizations or special cases to handle:

- The solution should handle cases where T contains elements not present in A by initializing required_elements to the number of unique elements in T.
- The count_T dictionary should be initialized with the counts of elements in T to efficiently count the occurrences of elements within the current window.

Here's the python solution:
```python
def main():
    try:
        n, m = map(int, input().split())
        A = list(map(int, input().split()))
        T = list(map(int, input().split()))
        result = shortest_subarray(n, m, A, T)
        if result == -1:
            print(-1)
        else:
            print(result[0], result[1])
    except Exception as e:
        # In a real application, you might want to log this error
        print(-1)

def shortest_subarray(n, m, A, T):
    count_T = {}
    for num in T:
        if num in count_T:
            count_T[num] += 1
        else:
            count_T[num] = 1
    
    required_elements = len(count_T)
    
    window_counts = {}
    formed_elements = 0
    
    min_length = float('inf')
    min_window = (-1, -1)
    
    left = 0
    for right in range(n):
        num = A[right]
        if num in count_T:
            window_counts[num] = window_counts.get(num, 0) + 1
            if window_counts[num] == count_T[num]:
                formed_elements += 1
        
        while formed_elements == required_elements and left <= right:
            if right - left + 1 < min_length:
                min_length = right - left + 1
                min_window = (left, right)
            elif right - left + 1 == min_length and left < min_window[0]:
                min_window = (left, right)
            
            num = A[left]
            if num in count_T:
                window_counts[num] -= 1
                if window_counts[num] < count_T[num]:
                    formed_elements -= 1
            left += 1
    
    if min_length == float('inf'):
        return -1
    else:
        return min_window

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given an n×m grid with:
- ‘.’ = free cell
- ‘#’ = wall
- ‘S’ = starting cell (there are at least two)

At time t = 0, each S “spreads” to its 4-neighbors (up/down/left/right) simultaneously each minute, never crossing walls. Define a meeting as two or more different sources arriving at the same cell in the same minute. Find the minimal t at which a meeting happens. If a meeting occurs, also return any one such cell.

Input
- First line: n m (1 ≤ n, m ≤ 1000)
- Next n lines: grid of length m consisting of characters in {‘.’, ‘#’, ‘S’}
- There are at least two ‘S’ cells.

Output
- If a meeting occurs: print t r c (1-based row and column of any meeting cell at time t)
- If no meeting occurs: print -1

Examples
1)
Input
3 3
S..
...
..S
Output
2 2 2
Explanation: Both sources reach (2,2) at t=2.

2)
Input
3 3
S##
###
##S
Output
-1
Explanation: Sources are in disconnected regions; they never meet.

SOLUTION:

# Problem Understanding

The problem requires finding the minimal time `t` at which two or more different sources (`'S'`) spread to the same cell in a grid and reporting any one such cell. The grid contains free cells (`'.'`), walls (`'#'`), and starting cells (`'S'`). The sources spread to their 4-neighbors (up, down, left, right) simultaneously each minute, without crossing walls.

Key constraints and edge cases:

- The grid size is `n x m`, where `1 ≤ n, m ≤ 1000`.
- There are at least two `'S'` cells.
- The grid contains only characters in `{'.', '#', 'S'}`.
- Sources spread to their 4-neighbors simultaneously each minute.
- Sources do not cross walls.

Input: The grid size `n` and `m`, followed by the grid of size `n x m`.

Output: If a meeting occurs, print the time `t` and the 1-based row and column of any meeting cell. If no meeting occurs, print `-1`.

Subtle requirements:

- The problem requires finding the minimal time `t` at which a meeting happens.
- The output should be in the format `t r c`, where `r` and `c` are 1-based row and column indices.

# Key Insights

- The problem can be solved using a breadth-first search (BFS) algorithm to simulate the spread of sources.
- We need to keep track of the time `t` and the cell coordinates for each source.
- A separate data structure is required to store the cells that are visited by each source to detect meetings.
- We can use a queue to store the cells to be visited next and a set or matrix to keep track of visited cells.
- To detect meetings, we need to check if a cell has been visited by another source at the same time `t`.
- We can maintain a separate data structure to store the visitation information for each cell.
- If a cell has been visited by another source at the same time `t`, we have found a meeting point.

Critical examples or test cases:

- The example where two sources meet at a cell (e.g., `3 3`, `S..`, `...`, `..S`) helped understand the problem.
- The example where sources are in disconnected regions (e.g., `3 3`, `S##`, `###`, `##S`) helped understand the edge case where no meeting occurs.

# Solution Strategy

1. Initialize a queue with all `'S'` cells and their corresponding source IDs.
2. Mark the `'S'` cells as visited by their respective source IDs.
3. Perform BFS:
   - Dequeue a cell and its source ID.
   - Enqueue its unvisited neighbors that are not walls.
   - Mark the neighbors as visited by the current source ID.
   - Check if a neighbor has been visited by another source at the same time `t`. If so, report the meeting point.
4. Repeat step 3 until a meeting is found or the queue is empty.
5. If a meeting is found, return the time `t` and the 1-based row and column of the meeting cell.
6. If the queue is empty and no meeting is found, return `-1`.

Important optimizations or special cases:

- Using a queue to store cells to be visited next allows for efficient BFS.
- Maintaining a separate data structure to store visitation information for each cell enables detection of meetings.
- Checking for meetings at each step ensures that the minimal time `t` is found.

Here's the python solution:
```python
from collections import deque

def main():
    n, m = map(int, input().split())
    grid = [list(input().strip()) for _ in range(n)]
    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]
    sources = []
    for i in range(n):
        for j in range(m):
            if grid[i][j] == 'S':
                sources.append((i, j))

    queue = deque()
    visited = {}
    for idx, (x, y) in enumerate(sources):
        queue.append((x, y, 0, idx))  # x, y, time, source_id
        if (x, y) not in visited:
            visited[(x, y)] = {}
        visited[(x, y)][idx] = 0

    meeting_info = None
    source_ids = list(range(len(sources)))
    while queue:
        x, y, t, source_id = queue.popleft()
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 <= nx < n and 0 <= ny < m and grid[nx][ny] != '#':
                if (nx, ny) not in visited or source_id not in visited[(nx, ny)]:
                    if (nx, ny) not in visited:
                        visited[(nx, ny)] = {}
                    visited[(nx, ny)][source_id] = t + 1
                    queue.append((nx, ny, t + 1, source_id))
                    if len(visited[(nx, ny)]) > 1:
                        all_source_ids = list(visited[(nx, ny)].keys())
                        if t + 1 == visited[(nx, ny)][all_source_ids[0]] and all(visited[(nx, ny)][sid] == t + 1 for sid in all_source_ids):
                            meeting_info = (t + 1, nx + 1, ny + 1)
                            break
        if meeting_info is not None:
            break

    if meeting_info:
        print(*meeting_info)
    else:
        print(-1)

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given an H×W grid containing:
- '.' empty cells
- '#' walls (impassable)
- 'S' starting cells (one or more)

At minute 0, all 'S' cells are filled. Each minute, the fill spreads to the 4-neighbors (up/down/left/right) of any already-filled cell, but cannot pass through '#'.

Task:
- Compute the minimum minutes required to fill all reachable '.' cells (i.e., the maximum distance from any 'S' to a reachable '.' via 4-neighbor steps).
- Also compute how many '.' cells are unreachable from every 'S'.

Input
- First line: H W (integers)
- Next H lines: each a string of length W over the alphabet {'.', '#', 'S'}

Output
- Print two integers: T U
  - T = minimum minutes to fill all reachable '.' cells
    - If there are no 'S' cells:
      - If there exist '.' cells: T = -1
      - Otherwise (no '.' at all): T = 0
  - U = count of '.' cells that are unreachable from all 'S'

Example
Input
3 3
S..
...
..S
Output
2 0
Explanation: The farthest '.' from the nearest 'S' is at distance 2; all '.' are reachable.

SOLUTION:

# Problem Understanding

The problem requires computing the minimum minutes required to fill all reachable '.' cells in a given grid and counting the number of '.' cells that are unreachable from all 'S' cells. The grid contains 'S' cells (starting cells), '.' cells (empty cells), and '#' cells (walls).

Key constraints and edge cases:

- The grid is of size H x W.
- The grid contains '.' empty cells, '#' walls (impassable), and 'S' starting cells (one or more).
- At minute 0, all 'S' cells are filled.
- Each minute, the fill spreads to the 4-neighbors (up/down/left/right) of any already-filled cell, but cannot pass through '#'.
- If there are no 'S' cells, and there are '.' cells, the output should be -1 for the minimum minutes.
- If there are no 'S' cells and no '.' cells, the output should be 0 for the minimum minutes.
- The output should include the count of '.' cells that are unreachable from all 'S' cells.

Input: A grid of size H x W containing 'S', '.', and '#' cells.
Output: Two integers, T and U, where T is the minimum minutes required to fill all reachable '.' cells, and U is the count of '.' cells that are unreachable from all 'S' cells.

Subtle requirements:

- The problem requires handling the case where there are multiple 'S' cells.
- The problem requires counting the number of '.' cells that are unreachable from all 'S' cells.

# Key Insights

- The problem can be solved using a breadth-first search (BFS) algorithm.
- BFS is suitable because it explores the grid level by level, which corresponds to the minutes passing in the problem.
- By starting BFS from all 'S' cells simultaneously, we can track the distance (in minutes) to fill each reachable '.' cell.
- The maximum distance from any 'S' to a reachable '.' cell represents the minimum minutes required to fill all reachable '.' cells.
- We need to keep track of visited cells to avoid revisiting them and to count the number of '.' cells that are unreachable from all 'S' cells.
- If there are no 'S' cells, we need to handle this as a special case and output -1 if there are '.' cells, or 0 if there are no '.' cells.
- We need to iterate through the grid to count the total number of '.' cells and compare this with the number of '.' cells that are visited during BFS to find the count of unreachable '.' cells.

# Solution Strategy

1. Read the input grid and its dimensions (H and W).
2. Initialize a visited matrix to keep track of visited cells.
3. Initialize a queue for BFS with all 'S' cells at minute 0.
4. Mark all 'S' cells as visited.
5. Perform BFS, exploring the grid level by level, and update the maximum distance (in minutes) to fill any reachable '.' cell.
6. During BFS, keep track of the number of '.' cells that are visited.
7. After BFS, count the total number of '.' cells in the grid.
8. Compare the total number of '.' cells with the number of '.' cells visited during BFS to find the count of unreachable '.' cells.
9. If there are no 'S' cells, handle this as a special case: output -1 if there are '.' cells, or 0 if there are no '.' cells.
10. Output the minimum minutes required to fill all reachable '.' cells and the count of '.' cells that are unreachable from all 'S' cells.

Here's the python solution:
```python
from collections import deque

def main():
    H, W = map(int, input().split())
    grid = [list(input().strip()) for _ in range(H)]
    max_distance, unreachable = solve(H, W, grid)
    print(max_distance, unreachable)

def solve(H, W, grid):
    queue = deque()
    visited = [[False for _ in range(W)] for _ in range(H)]
    S_count = 0
    dot_count = 0
    
    for i in range(H):
        for j in range(W):
            if grid[i][j] == 'S':
                queue.append((i, j, 0))
                visited[i][j] = True
                S_count += 1
            if grid[i][j] == '.':
                dot_count += 1
                
    if S_count == 0:
        if dot_count > 0:
            return -1, dot_count
        else:
            return 0, 0
    
    max_distance = 0
    reachable_dots = 0
    
    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]
    while queue:
        x, y, distance = queue.popleft()
        if grid[x][y] == '.':
            reachable_dots += 1
        max_distance = max(max_distance, distance)
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 <= nx < H and 0 <= ny < W and grid[nx][ny] != '#' and not visited[nx][ny]:
                queue.append((nx, ny, distance + 1))
                visited[nx][ny] = True
                
    unreachable_dots = dot_count - reachable_dots
    return max_distance, unreachable_dots

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Given N followed by N integers, output how many distinct integers appear.

Input
- Line 1: integer N (1 ≤ N ≤ 200000)
- Line 2: N space-separated integers ai (−10^9 ≤ ai ≤ 10^9)

Output
- A single integer: the number of distinct values.

Examples
1)
Input
7
1 2 2 1 3 4 4
Output
4

2)
Input
5
-1 -1 -1 -1 -1
Output
1

SOLUTION:

# Problem Understanding

The problem requires counting the number of distinct integers in a given list of N integers.

Key constraints and edge cases:
- N is an integer between 1 and 200,000 (inclusive).
- The integers in the list can range from -10^9 to 10^9.
- The input list may contain duplicate integers.
- The input list may contain negative integers.

Input: 
- The first line contains the value of N.
- The second line contains N space-separated integers.

Output: 
- A single integer representing the number of distinct integers in the input list.

Subtle requirements or implicit constraints:
- The input list is not sorted.
- The order of the integers in the output does not matter.

# Key Insights

Crucial observations that lead to the solution:
- To count distinct integers, we need to remove duplicates from the list.
- Using a data structure that automatically removes duplicates, such as a set, can simplify the problem.
- The size of the set after adding all integers will represent the number of distinct integers.

Patterns or properties being used:
- The property of a set data structure to store unique elements.

Key algorithmic insights:
- Reading the input N to know how many integers to expect.
- Reading N integers into a data structure that removes duplicates.
- Outputting the size of the set as the number of distinct integers.

Important realizations:
- The need to correctly read the input N and the N integers that follow.
- The need to handle large values of N (up to 200,000).

Critical examples or test cases:
- Example 1: Input: 7, followed by 1 2 2 1 3 4 4. Expected Output: 4.
- Example 2: Input: 5, followed by -1 -1 -1 -1 -1. Expected Output: 1.

# Solution Strategy

Steps to solve the problem:
1. Read the input N, representing the number of integers to follow.
2. Read N space-separated integers into a list or array.
3. Store the integers in a set data structure to remove duplicates.
4. Output the size of the set as the number of distinct integers.

Critical details:
- Correctly reading the input N and the N integers that follow.
- Using a set data structure to remove duplicates.

Important optimizations or special cases:
- Using a set data structure to automatically remove duplicates, rather than manually checking for duplicates.

Here's the python solution:
```python
def count_distinct_integers():
    try:
        N = int(input().strip())
        integers = list(map(int, input().strip().split()))
        distinct_integers = set(integers)
        return len(distinct_integers)
    except Exception as e:
        # In a real application, you'd log this error
        return None

if __name__ == "__main__":
    result = count_distinct_integers()
    if result is not None:
        print(result)
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given a directed graph with N nodes labeled 1..N and M edges. Determine whether the graph:
- has a cycle (no topological order),
- has more than one valid topological order, or
- has exactly one unique topological order.

If a unique topological order exists, output that order. Otherwise:
- output IMPOSSIBLE if the graph contains a cycle,
- output AMBIGUOUS if the graph is acyclic but the topological order is not unique.

Input
- Line 1: N M
- Next M lines: u v (1 ≤ u, v ≤ N), meaning a directed edge u → v

Output
- If unique topological order exists: print N space-separated integers (the order)
- Else if acyclic but not unique: print AMBIGUOUS
- Else (cycle exists): print IMPOSSIBLE

Examples

Example 1 (unique)
Input
4 3
1 2
2 3
3 4

Output
1 2 3 4

Example 2 (ambiguous)
Input
3 2
1 3
2 3

Output
AMBIGUOUS

Example 3 (impossible: cycle)
Input
3 3
1 2
2 3
3 1

Output
IMPOSSIBLE

SOLUTION:

# Problem Understanding

The problem requires determining whether a given directed graph has a cycle, has more than one valid topological order, or has exactly one unique topological order. If a unique topological order exists, the task is to output that order. Otherwise, the output should be "IMPOSSIBLE" if the graph contains a cycle, or "AMBIGUOUS" if the graph is acyclic but the topological order is not unique.

Key constraints and edge cases:

- The graph is a directed graph.
- The graph has N nodes labeled 1..N.
- The graph has M edges.
- The input is given as N M, followed by M lines representing the edges (u v), where 1 ≤ u, v ≤ N.
- The output should be either a unique topological order (N space-separated integers), "IMPOSSIBLE", or "AMBIGUOUS".

Input: N M, followed by M lines representing the edges (u v).
Output: A unique topological order, "IMPOSSIBLE", or "AMBIGUOUS".

Subtle requirements or implicit constraints:

- The graph may contain nodes with no incoming edges.
- The graph may contain cycles.
- The topological order should be output in the correct order (i.e., for every edge u -> v, u comes before v).

# Key Insights

Crucial observations that lead to the solution:

1. **Topological Sorting**: A topological sort is an ordering of the vertices in a directed acyclic graph (DAG) such that for every directed edge u -> v, vertex u comes before v in the ordering.
2. **Kahn's Algorithm**: Kahn's algorithm is suitable for this task as it can help detect cycles and can be modified to check for uniqueness of the topological order.
3. **Cycle Detection**: If at any point in Kahn's algorithm, there are no nodes with an in-degree of 0 and there are still nodes remaining, it indicates a cycle because the remaining nodes are not reachable in a topological order.
4. **Uniqueness Check**: To check for uniqueness, we need to verify if at any step during the topological sort, there is more than one node with an in-degree of 0. If so, and these nodes are not dependent on each other in a way that would force a specific order, it could lead to multiple valid topological orders.
5. **In-degree Tracking**: Maintaining the in-degree of each node helps in identifying nodes that are ready to be included in the topological order (i.e., nodes with an in-degree of 0).
6. **Queue Management**: Using a queue to store nodes with an in-degree of 0 allows for efficient processing and checking for uniqueness.

Patterns or properties being used:

- The property of a DAG having a topological order.
- Kahn's algorithm for topological sorting.

Key algorithmic insights:

- Using Kahn's algorithm to detect cycles and check for uniqueness.
- Modifying Kahn's algorithm to track nodes with in-degree 0 and checking for multiple such nodes at any step.

Important realizations:

- The need to check for cycles before checking for uniqueness.
- The importance of tracking in-degrees and using a queue for efficient processing.

Critical examples or test cases that led to insights:

- Example 1: A simple DAG with a unique topological order.
- Example 2: A DAG with multiple valid topological orders.
- Example 3: A graph with a cycle.

# Solution Strategy

Steps to solve this problem:

1. **Read Input**: Read N and M, and then the M edges to construct the graph and calculate the in-degree of each node.
2. **Apply Kahn's Algorithm**: Initialize a queue with nodes having an in-degree of 0.
3. **Process Nodes**: While the queue is not empty, dequeue a node, add it to the topological order, and decrease the in-degree of its neighbors. If a neighbor's in-degree becomes 0, add it to the queue.
4. **Check for Cycle**: If after processing all nodes, there are still nodes remaining (i.e., not all nodes were included in the topological order), it indicates a cycle, so output "IMPOSSIBLE".
5. **Check for Uniqueness**: During Kahn's algorithm, if at any point the queue contains more than one node, it indicates the potential for multiple valid topological orders, so mark the order as not unique.
6. **Output**: If a cycle is detected, output "IMPOSSIBLE". If the topological order is not unique, output "AMBIGUOUS". Otherwise, output the unique topological order.

Here's the python solution:
```python
from collections import defaultdict, deque

def topological_sort(N, edges):
    graph = defaultdict(list)
    in_degree = [0] * (N + 1)
    
    for u, v in edges:
        graph[u].append(v)
        in_degree[v] += 1
    
    queue = deque([i for i in range(1, N + 1) if in_degree[i] == 0])
    topological_order = []
    ambiguous = False
    
    while queue:
        if len(queue) > 1:
            ambiguous = True
        node = queue.popleft()
        topological_order.append(node)
        for neighbor in graph[node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)
    
    if len(topological_order) != N:
        return "IMPOSSIBLE"
    else:
        if ambiguous:
            return "AMBIGUOUS"
        else:
            return ' '.join(map(str, topological_order))

def main():
    try:
        N, M = map(int, input().split())
        edges = []
        for _ in range(M):
            u, v = map(int, input().split())
            edges.append((u, v))
        print(topological_sort(N, edges))
    except Exception as e:
        # In a real application, you'd log this error
        pass

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Task:
Given a string S and an integer K, find the longest contiguous substring of S containing at most K distinct characters. If multiple substrings share the maximum length, choose the one with the smallest starting index (0-based). Output the starting index and the length.

Input:
- Line 1: S (a string of length n, 0 ≤ n ≤ 200000). Treat characters by exact equality (e.g., byte or code point equality); no normalization required.
- Line 2: K (integer, 0 ≤ K ≤ n)

Output:
- Two space-separated integers: start length
  - start: the 0-based starting index of the chosen substring
  - length: the substring length
- If K = 0 or S is empty, output “0 0”.

Examples:
1)
Input:
aabacbebebe
3
Output:
4 7
Explanation:
One optimal substring is “cbebebe” (indices 4..10), which has at most 3 distinct characters.

2)
Input:
aaaa
1
Output:
0 4
Explanation:
The entire string has only one distinct character.

3)
Input:
abc
0
Output:
0 0
Explanation:
No non-empty substring can have ≤ 0 distinct characters.

SOLUTION:

# Problem Understanding

The problem requires finding the longest contiguous substring of a given string S that contains at most K distinct characters. If multiple substrings share the maximum length, the one with the smallest starting index (0-based) should be chosen.

Key constraints and edge cases:

- The input string S has a length n, where 0 ≤ n ≤ 200000.
- The integer K is within the range 0 ≤ K ≤ n.
- Characters in S are treated by exact equality (e.g., byte or code point equality); no normalization is required.
- If K = 0 or S is empty, the output should be "0 0".

Input: A string S and an integer K.
Output: Two space-separated integers representing the starting index and the length of the chosen substring.

Subtle requirements or implicit constraints:

- The problem requires considering contiguous substrings only.
- The output should be based on 0-based indexing.

# Key Insights

Crucial observations that lead to the solution:

1. The problem can be solved using a sliding window approach, where the window represents the current substring being considered.
2. Maintaining a dictionary (or frequency map) of characters within the current window is essential to count the number of distinct characters.
3. The sliding window approach involves expanding the window to the right and shrinking it from the left when necessary to maintain the condition of having at most K distinct characters.
4. The maximum length substring and its starting index need to be tracked during the process.
5. Edge cases, such as K = 0 or an empty string S, should be handled separately.

Patterns or properties being used:

- The sliding window technique is used to efficiently explore the substrings of S.
- The frequency map of characters within the window helps in determining the number of distinct characters.

Key algorithmic insights:

- The algorithm involves iterating over the string S, expanding the window to the right, and updating the frequency map.
- When the number of distinct characters exceeds K, the window is shrunk from the left until the condition is met again.

Important realizations:

- The need to maintain a frequency map of characters within the window to efficiently count distinct characters.
- The importance of tracking the maximum length substring and its starting index.

Critical examples or test cases that led to insights:

- The example with S = "aabacbebebe" and K = 3 helped illustrate the need to slide the window and update the frequency map.
- The case with K = 0 or an empty S highlighted the need to handle these edge cases separately.

# Solution Strategy

Steps to solve this problem:

1. Handle edge cases: If K = 0 or S is empty, return "0 0".
2. Initialize variables: start = 0, max_len = 0, max_start = 0, and an empty dictionary char_count to store the frequency of characters within the window.
3. Iterate over the string S using the right pointer of the sliding window.
4. For each character at the right pointer, update char_count and the count of distinct characters.
5. If the number of distinct characters exceeds K, slide the window to the left by incrementing the start pointer and update char_count accordingly.
6. During the process, track the maximum length substring and its starting index.
7. After iterating over S, return the starting index and length of the maximum length substring.

Important optimizations or special cases to handle:

- Handling edge cases (K = 0 or S is empty) separately to avoid unnecessary computations.
- Using a dictionary to store the frequency of characters within the window for efficient counting of distinct characters.

Here's the python solution:
```python
def longest_substring(S, K):
    if K == 0 or not S:
        return "0 0"
    
    start = 0
    max_len = 0
    max_start = 0
    char_count = {}
    distinct_chars = 0
    
    for end in range(len(S)):
        char = S[end]
        if char not in char_count:
            char_count[char] = 0
        char_count[char] += 1
        
        if char_count[char] == 1:
            distinct_chars += 1
        
        while distinct_chars > K:
            left_char = S[start]
            char_count[left_char] -= 1
            if char_count[left_char] == 0:
                distinct_chars -= 1
            start += 1
        
        if end - start + 1 > max_len:
            max_len = end - start + 1
            max_start = start
    
    return f"{max_start} {max_len}"

def main():
    try:
        S = input().strip()
        K = int(input().strip())
        print(longest_substring(S, K))
    except Exception as e:
        # In a real application, you might log this error
        pass

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Task:
Given a string S and an integer K, find the longest contiguous substring of S containing at most K distinct characters. If multiple substrings share the maximum length, choose the one with the smallest starting index (0-based). Output the starting index and the length.

Input:
- Line 1: S (a string of length n, 0 ≤ n ≤ 200000). Treat characters by exact equality (e.g., byte or code point equality); no normalization required.
- Line 2: K (integer, 0 ≤ K ≤ n)

Output:
- Two space-separated integers: start length
  - start: the 0-based starting index of the chosen substring
  - length: the substring length
- If K = 0 or S is empty, output “0 0”.

Examples:
1)
Input:
aabacbebebe
3
Output:
4 7
Explanation:
One optimal substring is “cbebebe” (indices 4..10), which has at most 3 distinct characters.

2)
Input:
aaaa
1
Output:
0 4
Explanation:
The entire string has only one distinct character.

3)
Input:
abc
0
Output:
0 0
Explanation:
No non-empty substring can have ≤ 0 distinct characters.

SOLUTION:

# Problem Understanding

The problem requires finding the longest contiguous substring of a given string S that contains at most K distinct characters. If multiple substrings share the maximum length, the one with the smallest starting index (0-based) should be chosen.

Key constraints and edge cases:

- The input string S has a length n, where 0 ≤ n ≤ 200000.
- The integer K is within the range 0 ≤ K ≤ n.
- Characters in S are treated by exact equality (e.g., byte or code point equality); no normalization is required.
- If K = 0 or S is empty, the output should be "0 0".

Input: A string S and an integer K.
Output: Two space-separated integers representing the starting index and the length of the chosen substring.

Subtle requirements or implicit constraints:

- The problem requires considering contiguous substrings only.
- The output should be based on 0-based indexing.

# Key Insights

Crucial observations that lead to the solution:

1. The problem can be solved using a sliding window approach, where the window represents the current substring being considered.
2. Maintaining a dictionary (or frequency map) of characters within the current window is essential to count the number of distinct characters.
3. The sliding window approach involves expanding the window to the right and shrinking it from the left when necessary to maintain the condition of having at most K distinct characters.
4. The maximum length substring and its starting index need to be tracked during the process.
5. Edge cases, such as K = 0 or an empty string S, should be handled separately.

Patterns or properties being used:

- The sliding window technique is used to efficiently explore the substrings of S.
- The frequency map of characters within the window helps in determining the number of distinct characters.

Key algorithmic insights:

- The algorithm involves iterating over the string S, expanding the window to the right, and updating the frequency map.
- When the number of distinct characters exceeds K, the window is shrunk from the left until the condition is met again.

Important realizations:

- The need to maintain a frequency map of characters within the window to efficiently count distinct characters.
- The importance of tracking the maximum length substring and its starting index.

Critical examples or test cases that led to insights:

- The example with S = "aabacbebebe" and K = 3 helped illustrate the need to slide the window and update the frequency map.
- The case with K = 0 or an empty S highlighted the need to handle these edge cases separately.

# Solution Strategy

Steps to solve this problem:

1. Handle edge cases: If K = 0 or S is empty, return "0 0".
2. Initialize variables: start = 0, max_len = 0, max_start = 0, and an empty dictionary char_count to store the frequency of characters within the window.
3. Iterate over the string S using the right pointer of the sliding window.
4. For each character at the right pointer, update char_count and the count of distinct characters.
5. If the number of distinct characters exceeds K, slide the window to the left by incrementing the start pointer and update char_count accordingly.
6. During the process, track the maximum length substring and its starting index.
7. After iterating over S, return the starting index and length of the maximum length substring.

Important optimizations or special cases to handle:

- Handling edge cases (K = 0 or S is empty) separately to avoid unnecessary computations.
- Using a dictionary to store the frequency of characters within the window for efficient counting of distinct characters.

Here's the python solution:
```python
def longest_substring(S, K):
    if K == 0 or not S:
        return "0 0"
    
    start = 0
    max_len = 0
    max_start = 0
    char_count = {}
    distinct_chars = 0
    
    for end in range(len(S)):
        char = S[end]
        if char not in char_count:
            char_count[char] = 0
        char_count[char] += 1
        
        if char_count[char] == 1:
            distinct_chars += 1
        
        while distinct_chars > K:
            left_char = S[start]
            char_count[left_char] -= 1
            if char_count[left_char] == 0:
                distinct_chars -= 1
            start += 1
        
        if end - start + 1 > max_len:
            max_len = end - start + 1
            max_start = start
    
    return f"{max_start} {max_len}"

def main():
    try:
        S = input().strip()
        K = int(input().strip())
        print(longest_substring(S, K))
    except Exception as e:
        # In a real application, you might log this error
        pass

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given an array A of n positive integers and an integer k (1 ≤ k ≤ n). Split A into exactly k non-empty contiguous segments. The cost of a split is the maximum segment sum among the k segments. Find the minimum possible cost.

Input
- n k
- A1 A2 ... An

Output
- A single integer: the minimal possible maximum segment sum over all valid k-segment partitions.

Example
Input
5 2
7 2 5 10 8

Output
18

Explanation
Split as [7, 2, 5] (sum = 14) and [10, 8] (sum = 18). Any other split has a maximum segment sum ≥ 18, so the answer is 18.

SOLUTION:

# Problem Understanding

The problem requires finding the minimum possible maximum segment sum when splitting an array A of n positive integers into exactly k non-empty contiguous segments.

Key constraints and edge cases:

- The array A consists of n positive integers.
- The number of segments k is within the range 1 ≤ k ≤ n.
- The segments must be non-empty and contiguous.
- The input array A and the integer k are given as input.
- The output should be a single integer representing the minimum possible maximum segment sum.

Input: An array A of n positive integers and an integer k.
Output: The minimum possible maximum segment sum over all valid k-segment partitions.

Subtle requirements or implicit constraints:

- The segments must be non-empty, meaning each segment should contain at least one element.
- The segments must be contiguous, meaning the elements within a segment should be adjacent in the original array.

# Key Insights

Crucial observations that lead to the solution:

- The problem can be approached using a binary search strategy to find the minimum maximum segment sum.
- The minimum possible maximum segment sum should be within the range [max(A), sum(A)], where max(A) is the maximum element in A and sum(A) is the sum of all elements in A.
- For a given maximum segment sum, it is possible to check if it's possible to split the array into k segments such that the maximum segment sum is not exceeded.
- The binary search involves narrowing down the range of possible maximum segment sums until the minimum possible maximum segment sum is found.
- To check if a given maximum segment sum is valid, we can iterate through the array, accumulating elements into a segment until adding the next element would exceed the given maximum segment sum, and then start a new segment.
- If we can split the array into k or fewer segments, then the given maximum segment sum is a valid maximum segment sum.

Patterns or properties being used:

- The binary search strategy relies on the property that the minimum possible maximum segment sum lies within a specific range.
- The checking of a given maximum segment sum involves iterating through the array and counting the number of segments.

Critical examples or test cases that led to insights:

- The example given in the problem statement, where n = 5, k = 2, and A = [7, 2, 5, 10, 8], helps illustrate the problem and the required output.

# Solution Strategy

Steps to solve this problem:

1. Define a function that checks if it's possible to split the array into k segments with a maximum segment sum not exceeding a given value. This function will iterate through the array, accumulating elements into a segment until adding the next element would exceed the given maximum segment sum, and then start a new segment.
2. Initialize the range for the binary search, where the lower bound is the maximum element in A and the upper bound is the sum of all elements in A.
3. Perform a binary search within the defined range to find the minimum maximum segment sum. For each mid-value in the binary search, check if it's possible to split the array into k segments using the function defined in step 1.
4. If the check in step 3 returns true, update the upper bound to the mid-value. If it returns false, update the lower bound to the mid-value + 1.
5. Continue the binary search until the lower bound and upper bound converge to a single value, which is the minimum possible maximum segment sum.

Important optimizations or special cases to handle:

- The binary search strategy optimizes the search for the minimum maximum segment sum by reducing the search space at each step.
- The checking function is designed to handle the edge case where the number of segments k is equal to 1 or n.

Here's the python solution:
```python
def main():
    try:
        n, k = map(int, input().split())
        A = list(map(int, input().split()))
        print(split_array(n, k, A))
    except Exception as e:
        # In a real application, you might want to log this error
        pass

def split_array(n, k, A):
    def can_split(max_sum):
        segments = 1
        current_sum = 0
        for num in A:
            if current_sum + num > max_sum:
                segments += 1
                current_sum = num
                if segments > k:
                    return False
            else:
                current_sum += num
        return True
    
    left = max(A)
    right = sum(A)
    while left < right:
        mid = (left + right) // 2
        if can_split(mid):
            right = mid
        else:
            left = mid + 1
    return left

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given a circular array A of N integers and two integers K and D. A circular subarray is any non-empty contiguous arc along the circle (length 1 to N). A subarray is “nice” if:
- the number of distinct values in it is at most K, and
- max(subarray) − min(subarray) ≤ D.

Count how many circular subarrays are nice.

Input
- N K D
- A1 A2 … AN

Output
- A single integer: the number of nice circular subarrays.

Example 1
Input
4 2 1
1 2 2 3

Output
9

Explanation
All circular subarrays of length 1..4 starting at positions 1..4 that satisfy:
- distinct ≤ 2 and max−min ≤ 1.
Valid counts by start:
- Start 1: [1], [1,2], [1,2,2] → 3
- Start 2: [2], [2,2], [2,2,3] → 3
- Start 3: [2], [2,3] → 2
- Start 4: [3] → 1
Total = 3 + 3 + 2 + 1 = 9.

Example 2
Input
5 1 0
1 2 1 2 1

Output
5

Explanation
With K=1 and D=0, only single-element windows qualify, so there are exactly N = 5 nice subarrays.

SOLUTION:

# Problem Understanding

The problem requires counting the number of "nice" circular subarrays in a given circular array A of N integers. A circular subarray is considered "nice" if it satisfies two conditions:
1. The number of distinct values in the subarray is at most K.
2. The difference between the maximum and minimum values in the subarray is less than or equal to D.

Key constraints and edge cases:
- The array A is circular, meaning that the last element is connected to the first element.
- The length of the subarray can range from 1 to N.
- N, K, and D are given integers.
- The input array A contains N integers.

Input: N, K, D, and the array A of N integers.
Output: The total count of nice circular subarrays.

Subtle requirements or implicit constraints:
- The problem requires considering all possible circular subarrays, not just those that start within the first N elements.
- The conditions for a "nice" subarray must be satisfied simultaneously.

# Key Insights

Crucial observations that lead to the solution:
1. The problem can be approached using a sliding window technique, considering the circular nature of the array.
2. To handle the circular aspect, the array can be virtually extended by concatenating it with itself, but the solution will instead use modulo operations to wrap around the array.
3. Maintaining a frequency map of elements within the current window allows tracking the number of distinct elements.
4. Using two deques to keep track of the maximum and minimum elements within the current window enables efficient updates as the window expands or contracts.
5. The solution involves iterating over all possible start positions and expanding the window while counting the number of valid subarrays.
6. A subarray is valid if it has at most K distinct elements and the difference between its maximum and minimum values is within D.
7. The total count of nice subarrays is obtained by summing the counts for all start positions.

Important realizations:
- The need to efficiently update the maximum and minimum values within the window as it changes.
- The importance of considering the circular nature of the array when expanding the window.
- The requirement to count subarrays of different lengths starting from each position.

Critical examples or test cases:
- Example 1: N = 4, K = 2, D = 1, A = [1, 2, 2, 3] (Output: 9)
- Example 2: N = 5, K = 1, D = 0, A = [1, 2, 1, 2, 1] (Output: 5)

# Solution Strategy

Steps to solve the problem:
1. Iterate over all possible start positions in the array.
2. For each start position, initialize a window and expand it to the right (circularly).
3. Maintain a frequency map to track the number of distinct elements within the current window.
4. Use two deques to efficiently track the maximum and minimum elements within the window.
5. As the window expands, update the frequency map and the deques.
6. Check if the current window represents a valid (nice) subarray by verifying that it has at most K distinct elements and the difference between its maximum and minimum values is within D.
7. Count the number of valid subarrays for each start position.
8. Sum the counts for all start positions to obtain the total count of nice circular subarrays.

Important optimizations or special cases to handle:
- Efficiently updating the maximum and minimum values within the window using deques.
- Handling the circular nature of the array by using modulo operations to wrap around the array.
- Breaking the loop when the number of distinct elements exceeds K to avoid unnecessary further expansion of the window.

Here's the python solution:
```python
from collections import deque, defaultdict

def count_nice_subarrays(N, K, D, A):
    count = 0
    for start in range(N):
        freq_map = defaultdict(int)
        max_deque, min_deque = deque(), deque()
        left = 0
        distinct = 0
        
        for right in range(N):
            end = (start + right) % N
            freq_map[A[end]] += 1
            if freq_map[A[end]] == 1:
                distinct += 1
            
            while max_deque and A[max_deque[-1]] <= A[end]:
                max_deque.pop()
            max_deque.append(end)
            
            while min_deque and A[min_deque[-1]] >= A[end]:
                min_deque.pop()
            min_deque.append(end)
            
            while distinct > K or (max_deque and min_deque and A[max_deque[0]] - A[min_deque[0]] > D):
                if freq_map[A[start]] == 1:
                    distinct -= 1
                freq_map[A[start]] -= 1
                
                if max_deque and max_deque[0] == start:
                    max_deque.popleft()
                if min_deque and min_deque[0] == start:
                    min_deque.popleft()
                start = (start + 1) % N
                left += 1
            
            if max_deque and min_deque:
                count += right - left + 1
    
    return count

def main():
    try:
        N, K, D = map(int, input().split())
        A = list(map(int, input().split()))
        print(count_nice_subarrays(N, K, D, A))
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

- Task: Given an array of N integers and an integer K, count the number of contiguous subarrays whose sum equals K.

Input
- First line: two integers N and K
- Second line: N integers A1, A2, …, AN

Output
- One integer: the number of subarrays with sum exactly K

Examples
Example 1
Input:
5 2
1 1 1 1 1
Output:
4
Explanation:
Subarrays with sum 2: [A1..A2], [A2..A3], [A3..A4], [A4..A5].

Example 2
Input:
6 3
1 2 -1 2 -1 2
Output:
6
Explanation (one valid enumeration):
[A1..A2], [A1..A4], [A2..A4], [A3..A6], [A4..A6], [A5..A6].

SOLUTION:

# Problem Understanding

The task is to count the number of contiguous subarrays in a given array of N integers whose sum equals a given integer K.

Key constraints and edge cases:
- The input array can contain both positive and negative integers.
- The input array can be empty (N = 0).
- K can be 0.
- The subarrays must be contiguous.
- The input array is not guaranteed to be sorted.

Input: An array of N integers and an integer K.
Output: The number of contiguous subarrays with sum exactly K.

Subtle requirements or implicit constraints:
- The problem statement does not explicitly mention how to handle cases where N = 0, but it is implied that the output should be 0 in such cases.
- The problem statement does not explicitly mention how to handle cases where K = 0, but the solution should be able to handle this case correctly.

# Key Insights

Crucial observations that lead to the solution:
- To find the number of contiguous subarrays with sum K, we need to consider all possible subarrays and check if their sum equals K.
- A brute force approach would be to iterate over all pairs of start and end indices for subarrays, calculate the sum of each subarray, and check if it equals K. However, this approach is inefficient.
- A more efficient approach involves using a prefix sum technique, where we maintain a running sum of the elements as we iterate through the array.
- The prefix sum technique allows us to calculate the sum of any subarray in constant time.
- We can use a hashmap to store the frequency of prefix sums seen so far, which enables us to count the number of subarrays with sum K in a single pass through the array.
- For each element, we can check if there is a previous prefix sum that is equal to the current prefix sum minus K. If so, we can increment our count by the number of times this previous prefix sum has appeared.
- The hashmap should be initialized with a count of 1 for prefix sum 0, because the sum of an empty subarray is 0.

Critical examples or test cases that led to insights:
- Example 1: N = 5, K = 2, A = [1, 1, 1, 1, 1]. The subarrays with sum 2 are [A1..A2], [A2..A3], [A3..A4], [A4..A5].
- Example 2: N = 6, K = 3, A = [1, 2, -1, 2, -1, 2]. The subarrays with sum 3 are [A1..A2], [A1..A4], [A2..A4], [A3..A6], [A4..A6], [A5..A6].

# Solution Strategy

Steps to solve this problem:
1. Initialize a hashmap to store the frequency of prefix sums seen so far, with a count of 1 for prefix sum 0.
2. Initialize a variable to store the current prefix sum and a variable to store the count of subarrays with sum K.
3. Iterate through the input array, maintaining a running sum of the elements (prefix sum).
4. For each element, check if the hashmap contains a key equal to the current prefix sum minus K. If so, increment the count by the value associated with this key in the hashmap.
5. Update the hashmap with the current prefix sum, incrementing its count if it already exists in the hashmap, or adding it with a count of 1 if it does not.
6. After iterating through the entire array, return the count of subarrays with sum K.

Important optimizations or special cases to handle:
- The solution handles cases where N = 0 correctly, because the count will remain 0.
- The solution handles cases where K = 0 correctly, because the hashmap is initialized with a count of 1 for prefix sum 0.

Here's the python solution:
```python
def count_subarrays(n, k, arr):
    count = 0
    prefix_sum = 0
    sum_count = {0: 1}  # Initialize with 0 sum having a count of 1
    
    for num in arr:
        prefix_sum += num
        if prefix_sum - k in sum_count:
            count += sum_count[prefix_sum - k]
        sum_count[prefix_sum] = sum_count.get(prefix_sum, 0) + 1
    
    return count

def main():
    try:
        n, k = map(int, input().split())
        arr = list(map(int, input().split()))
        print(count_subarrays(n, k, arr))
    except Exception as e:
        # In a real application, you'd log this error
        pass

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Given a non-empty string s, find:
1) the 0-based index i (0 ≤ i < |s|) at which the lexicographically smallest rotation of s begins, and
2) the smallest rotation string itself.

Definitions
- A rotation of s of length n at index i is s[i..n-1] + s[0..i-1].
- Lexicographic order is standard dictionary order based on byte/character codes.
- If multiple rotations are equally minimal, choose the smallest starting index.

Input
- A single line with string s (1 ≤ |s| ≤ 200000).
- Characters may be ASCII letters; treat bytes/characters as-is.

Output
- Line 1: integer i, the 0-based starting index of the lexicographically minimal rotation.
- Line 2: the minimal rotation string.

Examples
1)
Input
bba
Output
2
abb

2)
Input
banana
Output
5
abanan

3)
Input
aaaa
Output
0
aaaa

SOLUTION:

# Problem Understanding

The problem requires finding the 0-based index at which the lexicographically smallest rotation of a given non-empty string begins, and the smallest rotation string itself.

Key constraints and edge cases:

- The input string is non-empty.
- The length of the string is between 1 and 200000 (inclusive).
- Characters in the string may be ASCII letters, and are treated as-is (i.e., based on their byte/character codes).
- If multiple rotations are equally minimal, the smallest starting index should be chosen.

Input: A single line with the input string s.
Output: Two lines - the first line contains the 0-based starting index of the lexicographically minimal rotation, and the second line contains the minimal rotation string itself.

Subtle requirements or implicit constraints:

- The lexicographic order is based on standard dictionary order using byte/character codes.
- The problem requires an efficient solution that can handle large input strings.

# Key Insights

Crucial observations that lead to the solution:

- A rotation of a string s at index i is essentially s[i..n-1] + s[0..i-1], where n is the length of s.
- To compare rotations lexicographically, we can compare the strings character by character from left to right.
- Generating all rotations explicitly might be inefficient for large strings.
- Using a modified comparison that can be applied to the original string s without actually generating the rotations is a more efficient approach.
- The problem can be solved using a variation of the algorithm that efficiently finds the lexicographically minimal rotation, possibly involving a comparison that can be done in a single pass through the string or using a data structure that allows for efficient comparison of rotations.
- Duplicating the string s (i.e., considering s + s) allows us to easily simulate rotations by considering substrings of length n starting at different indices.
- The algorithm to find the lexicographically smallest rotation involves iterating through the possible start indices and comparing the rotations without generating them explicitly.
- A more efficient algorithm for finding the lexicographically minimal rotation involves using a technique that avoids generating all rotations, such as using a modified comparison or a specific algorithm designed for this purpose like the Lyndon factorization or a simpler iterative comparison method.

Key algorithmic insights:

- The problem can be solved using a simple yet efficient algorithm that involves iterating through the string and comparing potential minimal rotations.
- The algorithm can be optimized by avoiding explicit generation of rotations and using a comparison method that can be applied directly to the original string or its duplicated version.

Important realizations:

- The problem requires an efficient solution that can handle large input strings.
- The lexicographic order is based on standard dictionary order using byte/character codes.

Critical examples or test cases that led to insights:

- The example with s = bba, where the rotations are ba, ab, and bb, and the lexicographically smallest rotation is ab starting at index 2.
- The example with s = banana, where the rotations are compared lexicographically to find the smallest rotation anaban starting at index 5.
- The example with s = aaaa, where all rotations are the same lexicographically, and the smallest starting index is 0.

# Solution Strategy

Steps to solve this problem:

1. Duplicate the input string s to form s + s, which allows us to simulate rotations by considering substrings of length n starting at different indices.
2. Initialize the minimum index to 0 and iterate through the possible start indices from 1 to n-1.
3. For each start index i, compare the substring of length n starting at i with the current minimum rotation (starting at the current minimum index).
4. Use a character-by-character comparison to determine if the new rotation is lexicographically smaller than the current minimum rotation.
5. If a smaller rotation is found, update the minimum index to the current start index i.
6. After iterating through all possible start indices, the minimum index will hold the 0-based starting index of the lexicographically minimal rotation.
7. Use the minimum index to generate the minimal rotation string by considering the substring of length n starting at the minimum index in the duplicated string s + s.
8. Output the minimum index on the first line and the minimal rotation string on the second line.

Important optimizations or special cases to handle:

- Avoid explicit generation of rotations by using a comparison method that can be applied directly to the original string or its duplicated version.
- Use a simple yet efficient algorithm that involves iterating through the string and comparing potential minimal rotations.

Here's the python solution:
```python
def find_min_rotation(s):
    n = len(s)
    s_s = s + s
    min_idx = 0
    for i in range(1, n):
        j = 0
        while j < n:
            if s_s[i + j] < s_s[min_idx + j]:
                min_idx = i
                break
            elif s_s[i + j] > s_s[min_idx + j]:
                break
            j += 1
    min_rotation = s_s[min_idx:min_idx + n]
    return min_idx, min_rotation

def main():
    s = input().strip()
    min_idx, min_rotation = find_min_rotation(s)
    print(min_idx)
    print(min_rotation)

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Given an integer n, a positive integer k, and an array of n integers a1, a2, …, an, find the maximum length L of a contiguous subarray whose sum is divisible by k. If no such subarray exists, output 0.

Input
- Line 1: n k
- Line 2: a1 a2 … an

Output
- A single integer: the maximum length L.

Examples
1)
Input
7 5
2 7 6 1 4 5 3
Output
5
Explanation: The subarray [2, 7, 6, 1, 4] has sum 20, divisible by 5.

2)
Input
5 3
-1 2 9 -4 5
Output
4
Explanation: The subarray [-1, 2, 9, -4] sums to 6, divisible by 3. Another valid length-4 subarray is [2, 9, -4, 5].

SOLUTION:

# Problem Understanding

The problem requires finding the maximum length of a contiguous subarray in a given array of integers such that the sum of the subarray is divisible by a given positive integer k.

Key constraints and edge cases:

- The input array has n integers.
- k is a positive integer.
- The subarray must be contiguous.
- If no such subarray exists, the output should be 0.
- The input array can contain both positive and negative integers.

Input: 
- The number of elements n in the array.
- The divisor k.
- The array of n integers.

Output: 
- The maximum length L of a contiguous subarray whose sum is divisible by k.

Subtle requirements or implicit constraints:

- The problem requires considering the divisibility of the sum of the subarray by k, not just the sum itself.
- The problem asks for the maximum length, so it is necessary to keep track of the maximum length found so far.

# Key Insights

Crucial observations that lead to the solution:

- To determine if a subarray's sum is divisible by k, we need to calculate the sum of the subarray and check its divisibility by k.
- Using a prefix sum approach can efficiently calculate the sum of any subarray.
- The divisibility of a sum by k can be checked using the modulus operator (%).
- If the remainder of the prefix sum divided by k is negative, adding k to it gives the equivalent positive remainder.
- Storing the prefix sum modulo k and its corresponding index can help in finding the maximum length of a subarray whose sum is divisible by k.
- If we encounter the same remainder again, it means we've found a subarray whose sum is divisible by k, and we can update the maximum length.

Patterns or properties being used:

- The property that the difference between two prefix sums having the same remainder when divided by k indicates a subarray whose sum is divisible by k.

Key algorithmic insights:

- Using a hashmap or dictionary to store the remainder of the prefix sum divided by k as the key and its corresponding index as the value.
- Iterating through the array, calculating the prefix sum, and updating the maximum length whenever we find a remainder that is already in the hashmap.

Important realizations:

- Initializing the hashmap with a remainder of 0 and an index of -1 is necessary to handle the case where the sum of the subarray starting from the beginning of the array is divisible by k.
- Updating the maximum length is done by comparing the current maximum length with the difference between the current index and the index stored in the hashmap for the corresponding remainder.

Critical examples or test cases that led to insights:

- The given examples in the problem statement, such as n = 7, k = 5, and a = [2, 7, 6, 1, 4, 5, 3], helped in understanding the problem and verifying the solution.

# Solution Strategy

Steps to solve this problem:

1. Read the input: the number of elements n in the array and the divisor k.
2. Read the array of n integers.
3. Initialize a hashmap with a remainder of 0 and an index of -1.
4. Iterate through the array, calculating the prefix sum at each step.
5. For each prefix sum, calculate its remainder when divided by k. If the remainder is negative, adjust it by adding k.
6. Check if the remainder is already in the hashmap. If it is, update the maximum length by comparing the current maximum length with the difference between the current index and the index stored in the hashmap for the corresponding remainder.
7. If the remainder is not in the hashmap, store it along with its corresponding index.
8. After iterating through the entire array, return the maximum length found.

Here's the python solution:
```python
def max_length_subarray(n, k, a):
    prefix_sum = 0
    max_length = 0
    remainder_index = {0: -1}
    
    for i in range(n):
        prefix_sum += a[i]
        remainder = prefix_sum % k
        if remainder < 0:
            remainder += k
        if remainder in remainder_index:
            max_length = max(max_length, i - remainder_index[remainder])
        else:
            remainder_index[remainder] = i
    return max_length

def main():
    try:
        n, k = map(int, input().split())
        a = list(map(int, input().split()))
        print(max_length_subarray(n, k, a))
    except Exception as e:
        # In a real application, you might want to log this error
        pass

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given a rectangular grid representing corridors (# = wall, . = open cell). There is exactly one starting cell S. Some cells contain lowercase keys ‘a’..‘j’ (at most 10 total). Some cells contain uppercase doors ‘A’..‘J’; door ‘A’ can be entered only after collecting key ‘a’, and similarly for other pairs.

In one move you may step to one of the four orthogonally adjacent cells (up, down, left, right) if it is inside the grid and not blocked by a wall or a locked door. You collect a key by stepping onto its cell. Keys are kept permanently once collected.

Goal
Compute the minimum number of moves required to collect all keys. If it is impossible, output -1.

Input
- First line: two integers H W (grid height and width)
- Next H lines: each a string of length W using characters:
  - ‘#’ wall, ‘.’ open cell, ‘S’ start (exactly one)
  - ‘a’..‘j’ keys, ‘A’..‘J’ doors

Output
- One integer: minimal number of moves to collect all keys, or -1 if impossible.

Example
Input
1 5
SaA.b

Output
4

SOLUTION:

# Problem Understanding

The problem requires finding the minimum number of moves to collect all keys in a given grid. The grid represents corridors, with '#' denoting walls, '.' denoting open cells, 'S' denoting the starting cell, 'a' to 'j' denoting keys, and 'A' to 'J' denoting doors.

Key constraints and edge cases:

- The grid is rectangular.
- There is exactly one starting cell 'S'.
- The grid contains at most 10 keys ('a' to 'j').
- Doors ('A' to 'J') can only be entered if the corresponding key is collected.
- Movement is restricted to four orthogonal directions (up, down, left, right).
- Movement is not allowed into walls or locked doors.
- Keys are collected by stepping onto their cells and are kept permanently.

Input: The grid height (H) and width (W), followed by H lines of the grid.

Output: The minimum number of moves required to collect all keys. If it's impossible to collect all keys, output -1.

Subtle requirements or implicit constraints:

- The grid is represented as a 2D grid of characters.
- The problem requires handling different states based on the keys collected so far.

# Key Insights

Crucial observations that lead to the solution:

- The problem can be approached using a breadth-first search (BFS) algorithm because it requires finding the shortest path.
- BFS needs to be adapted to keep track of the keys collected so far.
- The state in the search algorithm is not just the current position, but also the set of keys collected so far.
- A bitmask can be used to represent the set of keys collected, where each bit corresponds to one of the keys 'a' to 'j'.
- The grid needs to be parsed to understand the positions of 'S', keys, and doors.
- The BFS algorithm explores all possible next states from the current state (position and set of keys).
- For each state, it checks if the new position is valid (not a wall and not a locked door if the key is not collected).
- If a key is collected, the bitmask is updated accordingly.
- The algorithm terminates when it finds a state that has collected all keys or if the BFS queue is exhausted without finding such a state.

Patterns or properties being used:

- The BFS algorithm is used for finding the shortest path.
- Bitmask is used to represent the set of keys collected.

Key algorithmic insights:

- Using BFS with a state that includes the current position and the set of keys collected.
- Using a bitmask to efficiently represent the set of keys collected.

Important realizations:

- The need to adapt BFS to keep track of the keys collected.
- The use of a bitmask to represent the set of keys collected.

Critical examples or test cases that led to insights:

- The given example in the problem statement (1 5, SaA.b) helped illustrate the need to consider the state of keys collected and the use of BFS.

# Solution Strategy

Steps to solve this problem:

1. Parse the input grid to understand the positions of 'S', keys, and doors.
2. Initialize a BFS queue with the starting position and an empty set of keys (represented as a bitmask).
3. Perform BFS, exploring all possible next states from the current state (position and set of keys).
4. For each state, check if the new position is valid (not a wall and not a locked door if the key is not collected).
5. If a key is collected, update the bitmask accordingly.
6. Keep track of the number of moves for each state.
7. Terminate the BFS when it finds a state that has collected all keys or if the BFS queue is exhausted without finding such a state.
8. If a state with all keys collected is found, return the number of moves. If the BFS queue is exhausted without finding such a state, return -1.

Important optimizations or special cases to handle:

- Using a visited set to avoid revisiting the same state (position and set of keys).
- Checking if a door is locked by verifying if the corresponding key is in the bitmask.
- Calculating the total number of keys present in the grid to know when all keys are collected.

Here's the python solution:
```python
from collections import deque

def main():
    H, W = map(int, input().split())
    grid = [list(input().strip()) for _ in range(H)]
    print(shortest_path(H, W, grid))

def shortest_path(H, W, grid):
    start = None
    keys = {}
    total_keys = 0
    for i in range(H):
        for j in range(W):
            if grid[i][j] == 'S':
                start = (i, j)
            elif grid[i][j] in 'abcdefghij':
                key = grid[i][j]
                keys[key] = (i, j)
                total_keys += 1
    key_mask = 2 ** total_keys - 1
    
    queue = deque([(start[0], start[1], 0, 0)])  # (x, y, keys_collected, moves)
    visited = set([(start[0], start[1], 0)])
    
    while queue:
        x, y, keys_collected, moves = queue.popleft()
        if keys_collected == key_mask:
            return moves
        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
            nx, ny = x + dx, y + dy
            if 0 <= nx < H and 0 <= ny < W:
                cell = grid[nx][ny]
                if cell != '#':
                    new_keys_collected = keys_collected
                    if cell.islower() and cell in 'abcdefghij':
                        key_index = ord(cell) - ord('a')
                        new_keys_collected |= (1 << key_index)
                    if cell.isupper() and cell in 'ABCDEFGHIJ':
                        door_index = ord(cell) - ord('A')
                        if not (keys_collected & (1 << door_index)):
                            continue
                    new_state = (nx, ny, new_keys_collected)
                    if new_state not in visited:
                        queue.append((nx, ny, new_keys_collected, moves + 1))
                        visited.add(new_state)
    return -1

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given N integers a1..aN, each in the range [0, 2^B). For each of Q queries, you are given a bitmask S (0 ≤ S < 2^B). For each query, output the number of non-empty subsets of {a1..aN} whose bitwise OR is exactly S, modulo 1,000,000,007.

Input
- First line: B N Q
- Second line: N integers a1 a2 ... aN (0 ≤ ai < 2^B)
- Next Q lines: one integer S per line (0 ≤ S < 2^B)

Output
- Q lines: one integer per line — the number of non-empty subsets whose bitwise OR equals the given S, modulo 1,000,000,007.

Example
Input
3 4 3
1 2 4 3
3
7
1

Output
4
6
1

Explanation (for the example)
All non-empty subsets of [1,2,4,3] produce these ORs:
- OR=1: {1}
- OR=2: {2}
- OR=3: {3}, {1,2}, {1,3}, {2,3}
- OR=4: {4}
- OR=5: {1,4}
- OR=6: {2,4}
- OR=7: {3,4}, {1,2,4}, {1,3,4}, {2,3,4}, {1,2,3}, {1,2,3,4}
So answers: S=3 → 4, S=7 → 6, S=1 → 1.

SOLUTION:

# Problem Understanding

The problem requires finding the number of non-empty subsets of a given set of N integers, each in the range [0, 2^B), whose bitwise OR is exactly equal to a given query bitmask S, modulo 1,000,000,007.

Key constraints and edge cases:

- N integers are given, each in the range [0, 2^B)
- B is the number of bits in the binary representation of the integers
- Q queries are given, each with a bitmask S
- S is in the range [0, 2^B)
- The count of non-empty subsets whose bitwise OR equals S should be returned modulo 1,000,000,007
- The input integers and query bitmasks are non-negative integers

Input: B, N, Q, N integers, and Q query bitmasks
Output: The count of non-empty subsets whose bitwise OR equals each query bitmask S, modulo 1,000,000,007

Subtle requirements or implicit constraints:

- The count of subsets should be modulo 1,000,000,007 to avoid overflow
- The bitwise OR operation should be performed on the binary representation of the integers

# Key Insights

Crucial observations that lead to the solution:

- The problem can be solved using dynamic programming to precompute the counts of subsets that OR to each possible value S.
- The count of subsets that OR to S can be derived using the counts of subsets that OR to the subsets of S.
- The principle of inclusion-exclusion can be used to correctly count the number of subsets that OR to exactly S.
- The number of subsets of the numbers a1, a2, ..., aN that include a particular ai is 2^(N-1) if ai is less than or equal to S, and 0 otherwise.
- The count of non-empty subsets whose bitwise OR equals S can be computed using the formula: count = (sum of (-1)^(|S| - |T|) * 2^(number of ai that are subset of T)) for all T that are subsets of S.
- The number of ai that are subsets of T can be precomputed and stored in an array.

Patterns or properties being used:

- The bitwise OR operation and its properties
- The principle of inclusion-exclusion
- Dynamic programming to precompute the counts of subsets

Critical examples or test cases that led to insights:

- The example given in the problem statement, where N = 4, B = 3, and the integers are 1, 2, 4, 3.

# Solution Strategy

Steps to solve this problem:

1. Precompute the count of numbers ai that are subsets of each possible mask T in the range [0, 2^B).
2. Initialize an array dp of size 2^B to store the count of subsets that OR to each possible value S.
3. For each S in the range [0, 2^B), compute the count of subsets that OR to S using the precomputed counts and the principle of inclusion-exclusion.
4. For each query bitmask S, return the count of non-empty subsets whose bitwise OR equals S, modulo 1,000,000,007.
5. Use dynamic programming to efficiently compute the counts of subsets that OR to each possible value S.

Important optimizations or special cases to handle:

- Use modulo 1,000,000,007 to avoid overflow when computing the counts.
- Precompute the count of numbers ai that are subsets of each possible mask T to avoid redundant computation.
- Use the principle of inclusion-exclusion to correctly count the number of subsets that OR to exactly S.

Here's the python solution:
```python
def main():
    B, N, Q = map(int, input().split())
    A = list(map(int, input().split()))
    queries = []
    for _ in range(Q):
        queries.append(int(input()))
    MOD = 1000000007
    counts = precompute_counts(B, N, A)
    results = compute_results(B, counts, queries, MOD)
    for result in results:
        print(result)

def precompute_counts(B, N, A):
    MOD = 1000000007
    count = [0] * (1 << B)
    for ai in A:
        count[ai] += 1
    return count

def compute_results(B, counts, queries, MOD):
    results = []
    for S in queries:
        result = 0
        count_S = counts[S]
        for T in range(1 << B):
            if (T | S) == S:
                sign = (-1) ** (bin(S).count('1') - bin(T).count('1'))
                num_ai_subset_T = counts[T]
                result = (result + sign * (pow(2, num_ai_subset_T, MOD) - 1)) % MOD
        results.append(result)
    return results

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

You are given an integer array A of length n. For every contiguous subarray A[l..r], define its range as (max(A[l..r]) − min(A[l..r])). Compute the sum of ranges over all subarrays.

Input
- n (1 ≤ n ≤ 200000)
- A1 A2 … An (each |Ai| ≤ 10^9)

Output
- A single integer: the sum over all 1 ≤ l ≤ r ≤ n of (max(A[l..r]) − min(A[l..r])).

Examples
1)
Input
4
1 2 3 4
Output
10
Explanation:
Subarray ranges sum to 1+1+1+2+2+3 = 10.

2)
Input
3
1 3 2
Output
5
Explanation:
Length-2 ranges: 2, 1 → 3; length-3 range: 2 → total 5.

SOLUTION:

# Problem Understanding

The problem requires computing the sum of ranges over all subarrays in a given integer array A of length n. The range of a subarray A[l..r] is defined as (max(A[l..r]) − min(A[l..r])).

Key constraints and edge cases:

- The input array A has a length n, where 1 ≤ n ≤ 200000.
- Each element Ai in the array A is an integer, where |Ai| ≤ 10^9.
- The input array A can contain duplicate elements.
- The input array A can be empty (n = 0) in some edge cases, although this is not explicitly mentioned in the problem statement.

Input: An integer array A of length n.
Output: A single integer representing the sum of ranges over all subarrays.

Subtle requirements or implicit constraints:

- The problem statement does not explicitly mention how to handle the case when n = 0, but it is implied that the output should be 0 in such a case.
- The problem requires an efficient solution that can handle large inputs (n up to 200000).

# Key Insights

Crucial observations that lead to the solution:

1. The sum of ranges over all subarrays can be computed by considering the contribution of each element to the sum of ranges.
2. Each element Ai in the array can be a maximum or minimum for some subarrays.
3. To compute the sum of ranges, we need to calculate the sum of maximums over all subarrays and the sum of minimums over all subarrays.
4. The sum of ranges can then be obtained by subtracting the sum of minimums from the sum of maximums.
5. We can use a monotonic stack to efficiently count the number of subarrays for which each element is the maximum or minimum.

Patterns or properties being used:

- The monotonic stack property: for each element, we can find the previous and next larger (or smaller) element in the array.

Key algorithmic insights:

- Using a monotonic stack to count the number of subarrays for which each element is the maximum or minimum.
- Computing the sum of maximums and minimums over all subarrays by considering the contribution of each element.

Important realizations:

- We need to count the number of subarrays for which each element is the maximum or minimum.
- We can use the monotonic stack to find the previous and next larger (or smaller) element for each element.

Critical examples or test cases that led to insights:

- The examples provided in the problem statement, such as A = [1, 2, 3, 4] and A = [1, 3, 2], helped illustrate the concept of sum of ranges and the need to compute sum of maximums and minimums.

# Solution Strategy

Steps to solve this problem:

1. Initialize two arrays to store the sum of maximums and minimums over all subarrays.
2. Use a monotonic stack to count the number of subarrays for which each element is the maximum or minimum.
3. For each element Ai, find the previous and next larger (or smaller) element using the monotonic stack.
4. Compute the count of subarrays for which Ai is the maximum or minimum.
5. Update the sum of maximums and minimums using the counts obtained in step 4.
6. Compute the sum of ranges by subtracting the sum of minimums from the sum of maximums.

Important optimizations or special cases to handle:

- Handling the edge case when n = 0 (output should be 0).
- Using a monotonic stack to efficiently count the number of subarrays for which each element is the maximum or minimum.

Here's the python solution:
```python
def main():
    try:
        n = int(input().strip())
        A = list(map(int, input().strip().split()))
        if len(A) != n:
            # Handle input length mismatch if necessary
            pass
        print(sum_of_ranges(n, A))
    except Exception as e:
        # Handle exception if necessary
        pass

def sum_of_ranges(n, A):
    sum_max = sum_max_subarrays(n, A)
    sum_min = sum_min_subarrays(n, A)
    return sum_max - sum_min

def sum_max_subarrays(n, A):
    total = 0
    stack = []
    left = [-1] * n
    right = [n] * n
    
    for i in range(n):
        while stack and A[stack[-1]] <= A[i]:
            stack.pop()
        if stack:
            left[i] = stack[-1]
        stack.append(i)
    
    stack = []
    for i in range(n-1, -1, -1):
        while stack and A[stack[-1]] <= A[i]:
            stack.pop()
        if stack:
            right[i] = stack[-1]
        stack.append(i)
    
    for i in range(n):
        l = i - left[i]
        r = right[i] - i
        total += A[i] * l * r
    
    return total

def sum_min_subarrays(n, A):
    total = 0
    stack = []
    left = [-1] * n
    right = [n] * n
    
    for i in range(n):
        while stack and A[stack[-1]] >= A[i]:
            stack.pop()
        if stack:
            left[i] = stack[-1]
        stack.append(i)
    
    stack = []
    for i in range(n-1, -1, -1):
        while stack and A[stack[-1]] >= A[i]:
            stack.pop()
        if stack:
            right[i] = stack[-1]
        stack.append(i)
    
    for i in range(n):
        l = i - left[i]
        r = right[i] - i
        total += A[i] * l * r
    
    return total

if __name__ == "__main__":
    main()
```

- Total score: 0–100
- Evidence-first: prioritize objective signals (tests, timings, coverage, static analysis), then rubric-based reasoning
- Modular: enable/disable modules (e.g., Concurrency) per problem
- Fair across languages: normalize performance expectations by language profile or baseline
- Output: concise human summary + machine-readable JSON

Default 100‑point rubric (core + modules)
- Core (80 pts)
  - Correctness (tests): 60
  - Robustness & Edge Cases: 10
  - Code Quality & Design: 10
- Modules (20 pts total, allocate per problem)
  - Efficiency & Complexity: 0–20
  - API & Abstraction Design: 0–20
  - Concurrency & Parallelism: 0–20
  - Security & Resource Management: 0–10
  - Documentation & Style: 0–10

Recommended profiles (distribute module points to 20 total)
- Algorithmic: Efficiency 15, Docs/Style 5
- Library/API design: API & Abstraction 15, Docs/Style 5
- Concurrency: Concurrency 15, Efficiency 5
- Systems/IO: Security/Resources 10, Efficiency 10
- Data processing: Efficiency 10, Docs/Style 5, API & Abstraction 5

Category definitions and scoring rules
1) Correctness (60)
- Basis: test pass rate with weighted categories
  - Public/basic: 10%
  - Core functional: 50%
  - Edge/negative: 20%
  - Hidden/adversarial: 20%
- Optional multiplier: correctnessScore *= 0.75 + 0.25 × branchCoverage (cap at 1.0)

2) Robustness & Edge Cases (10)
- Score with partial credit (−2 to −4 per failed item)
  - Null/None/empty handling; invalid inputs; clear error messages
  - Bounds/overflow; numeric precision; timezone/locale where relevant
  - Determinism; no hidden global state; idempotence when required
  - Resource handling (files/sockets/processes): close/cleanup; timeouts
  - Input parsing tolerant to whitespace/format variants
- Strong, deliberate validation earns 9–10

3) Code Quality & Design (10)
- Cohesion/low coupling; separation of concerns (logic vs IO)
- Clear naming; small, focused functions; DRY; appropriate data structures
- Idiomatic use for the language; type safety or type hints where applicable
- Test quality (if included): clarity, coverage of contracts
- Deduct for god objects/functions, deep nesting, magic numbers, ad-hoc globals

4) Efficiency & Complexity (0–20 module)
- Meet required asymptotic complexity; assess empirical scaling across input sizes
- Time: slope-based inference; flag consistent TLE; language baseline aware
- Memory: avoid unnecessary copies/boxing; streaming vs materialization tradeoffs
- Guide:
  - Meets complexity and comfortably within limits: 16–20
  - Meets complexity but high constants/occasional TLE at max size: 12–15
  - One grade worse complexity but usable on smaller sizes: 6–11
  - Significantly suboptimal or frequent TLE/OOM: 0–5

5) API & Abstraction Design (0–20 module)
- Clear contracts; minimal surface area; sensible function signatures
- Encapsulation/immutability or purity where appropriate
- Extensibility via interfaces/traits/protocols/strategies
- Error taxonomy: exceptions vs error codes/results consistently used
- High marks for small composable abstractions that are easy to test

6) Concurrency & Parallelism (0–20 module)
- Correct synchronization/coordination; avoid deadlocks and data races
- Atomicity/visibility; safe publication; immutability for sharing
- Correct use of runtimes (threads, coroutines, async/await, actors)
- Interruption/cancellation; timeouts; backpressure where relevant
- Evidence: stress/race tests, randomized schedules; language-appropriate race tools if available

7) Security & Resource Management (0–10 module)
- Input sanitization; avoid injection/path traversal; least privilege defaults
- No unsafe deserialization; careful reflection/dynamic eval
- Secret handling: no logging secrets; constant-time compare where needed
- Proper closing of files/sockets; bounded buffers; timeouts; retry with jitter

8) Documentation & Style (0–10 module or 5 in API profile)
- Public API docs; edge cases noted; examples where relevant
- Consistent formatting per language standard; lints minimized
- Comments explain “why,” not “what”
- Deduct by violation count/severity, not personal style preference

If some scoring category is not applicable, give it a maximum score possible.

Use the above scoring methodology to score the solution of the following PROBLEM.

PROBLEM:

Given N half-open intervals [Li, Ri) on the integer number line and an integer K, compute the total length covered by at least K intervals.

Input
- N K
- N lines: Li Ri (0 ≤ Li < Ri ≤ 10^12)

Output
- A single integer: the total measure of points x where at least K intervals cover x, treating intervals as half-open [Li, Ri).

Example
Input
4 2
1 4
2 5
7 8
4 6

Output
3

Explanation
Coverage ≥ 2 occurs on [2,5), which has total length 3.

SOLUTION:

# Problem Understanding

The problem requires computing the total length covered by at least K intervals on the integer number line, given N half-open intervals [Li, Ri) and an integer K.

Key constraints and edge cases:

- The intervals are half-open, meaning they include the start point Li but exclude the end point Ri.
- The intervals are on the integer number line.
- 0 ≤ Li < Ri ≤ 10^12, indicating that the interval endpoints are non-negative integers and the end point Ri can be very large.
- N intervals are given, and the task is to find the total length covered by at least K intervals.
- K is a given integer that represents the minimum number of intervals required to cover a point.

Input: N intervals [Li, Ri) and an integer K.
Output: The total length covered by at least K intervals.

Subtle requirements or implicit constraints:

- The problem statement implies that the intervals may overlap, and the task is to find the total length of the regions that are covered by at least K intervals.
- The use of half-open intervals means that the end point Ri should not be counted towards the coverage.

# Key Insights

Crucial observations that lead to the solution:

1. To solve this problem, we need to consider the overlap between the given intervals.
2. A straightforward approach involves using a sweep line algorithm, where we consider the start and end points of all intervals and sweep through them in order.
3. By treating the start and end points as events and sorting them, we can keep track of the number of active intervals at any point.
4. When we encounter a start point, we increment the count of active intervals, and when we encounter an end point, we decrement the count.
5. By maintaining a count of active intervals, we can identify the regions where the count is at least K and calculate their total length.
6. To handle large interval endpoints (up to 10^12), we should avoid storing the entire range of numbers and instead focus on the events (start and end points) and the differences between consecutive events.
7. The key insight is that the total length covered by at least K intervals can be calculated by summing up the lengths of the regions where the count of active intervals is at least K.

Patterns or properties being used:

- The sweep line algorithm takes advantage of the fact that the intervals are half-open and that the start and end points can be treated as events.
- The algorithm relies on the property that the count of active intervals changes only at the event points (start and end points).

Critical examples or test cases that led to insights:

- The given example with N = 4, K = 2, and intervals [(1, 4), (2, 5), (7, 8), (4, 6)] helps illustrate the need to consider the overlap between intervals and to identify the regions where the count is at least K.

# Solution Strategy

Steps to solve this problem:

1. Create a list of events, where each event is either a start point or an end point of an interval.
2. Store the events as tuples, where the first element is the point (start or end), and the second element is a flag indicating whether it's a start point (1) or an end point (-1).
3. Sort the events based on the point value. If two events have the same point value, sort them based on the flag (start point comes before end point).
4. Initialize a counter to keep track of the number of active intervals and a variable to store the total length covered by at least K intervals.
5. Iterate through the sorted events. For each event, update the count of active intervals based on the flag (increment for start point, decrement for end point).
6. If the count of active intervals is at least K, add the length of the current region (difference between the current point and the previous point) to the total length.
7. To handle the case where the count of active intervals becomes at least K, keep track of the previous point where the count was at least K, and update the total length accordingly.
8. Return the total length covered by at least K intervals.

Here's the python solution:
```python
def total_length_covered(N, K, intervals):
    events = []
    for Li, Ri in intervals:
        events.append((Li, 1))
        events.append((Ri, -1))
    events.sort()
    count = 0
    total_length = 0
    prev_point = None
    for point, flag in events:
        if prev_point is not None and count >= K:
            total_length += point - prev_point
        count += flag
        prev_point = point
    return total_length

def main():
    try:
        N, K = map(int, input().split())
        intervals = []
        for _ in range(N):
            Li, Ri = map(int, input().split())
            intervals.append((Li, Ri))
        print(total_length_covered(N, K, intervals))
    except Exception as e:
        # In a real application, you might want to log this error
        pass

if __name__ == "__main__":
    main()
```
